{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Thy algorithm shalt not bear false witness\": An Evaluation of Multiclass Debiasing Methods on Word Embeddings\n",
    "\n",
    "\n",
    "This is the code to support the paper \"\"Thy algorithm shalt not bear false witness\": An Evaluation of Multiclass Debiasing Methods on Word Embeddings\"\n",
    " It shows, which experiments and debiasing techniques are applied to word embedding word2vec.\n",
    "The approaches are namely: \n",
    "\n",
    "Debiasing approaches: \n",
    "*   Conceptor Debiasing\n",
    "*   Hard Debiasing\n",
    "*   softWEAT\n",
    "\n",
    "Evaluation Metrics: \n",
    "*   Relative Negative Sentiment Bias(RNSB) Metric\n",
    "*   Mean Average Cosine Similarity (MAC) Metric\n",
    "*   Word Association Evaluation Test (WEAT) effect size Metric\n",
    "\n",
    "This Notebook refers to the contextNet embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed Packages\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import null_space as ns\n",
    "from statistics import mean\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadConceptNetVectors():\n",
    "    embeddings_dict = {}\n",
    "    with open(\"FILEPATH\\\\numberbatch-en.txt\", 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if isfloat(values[1]):\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                if len(vector) != 300:\n",
    "                    print('INVALID LENGTH', len(vector), word)\n",
    "                else:\n",
    "                    embeddings_dict[word] = vector\n",
    "                #print(word)\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVALID LENGTH 1 516782\n"
     ]
    }
   ],
   "source": [
    "model = loadConceptNetVectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Below the data for the debaising and evaluation methods are defined.\n",
    "\n",
    "The sets of words concerning a particular attribute class are taken from Popovic, Lemmerich and Strohmaier (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {'pleasant': ['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise', 'family', 'happy', 'laughter', 'paradise', 'vacation', 'joy', 'wonderful'],\n",
    "        'unpleasant': ['abuse', 'crash', 'filth', 'murder', 'sickness', 'accident', 'death', 'grief', 'poison', 'stink', 'assault', 'disaster', 'hatred', 'pollute', 'tragedy', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'kill', 'rotten', 'vomit', 'agony', 'prison', 'terrible', 'horrible'],\n",
    "        'instruments': ['bagpipe', 'cello', 'guitar', 'lute', 'trombone', 'banjo', 'clarinet', 'harmonica', 'mandolin', 'trumpet', 'bassoon', 'drum', 'harp', 'oboe', 'tuba', 'bell', 'fiddle', 'harpsichord', 'piano', 'viola', 'bongo', 'flute', 'horn', 'saxophone'],\n",
    "        'weapons': ['arrow', 'club', 'gun', 'missile', 'spear', 'dagger', 'harpoon', 'pistol', 'sword', 'blade', 'dynamite', 'hatchet', 'rifle', 'tank', 'bomb', 'firearm', 'knife', 'shotgun', 'teargas', 'cannon', 'grenade', 'mace', 'slingshot', 'whip'],\n",
    "        'career': ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career'],\n",
    "        'family': ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives'],\n",
    "        'science': ['science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment', 'astronomy'],\n",
    "        'art': ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture'], \n",
    "        'intellectual_words': ['resourceful', 'inquisitive', 'sagacious', 'inventive', 'adaptable', 'intuitive', 'analytical', 'imaginative', 'shrewd', 'thoughtful', 'smart', 'clever', 'brilliant', 'logical', 'intelligent', 'apt', 'genius', 'wise', 'stupid', 'dumb', 'dull', 'clumsy', 'foolish', 'naive', 'unintelligent'],\n",
    "        'appearance_words': ['alluring', 'voluptuous', 'blushing', 'homely', 'plump', 'sensual', 'gorgeous', 'slim', 'bald', 'athletic', 'fashionable', 'stout', 'ugly', 'muscular', 'slender', 'feeble', 'handsome', 'healthy', 'attractive', 'fat', 'weak', 'thin', 'pretty', 'beautiful', 'strong'], \n",
    "        'shy': ['soft', 'quiet', 'compromising', 'rational', 'calm', 'kind', 'agreeable', 'servile', 'pleasant', 'cautious', 'friendly', 'supportive', 'nice', 'mild', 'demure', 'passive', 'indifferent'],\n",
    "        'aggressive': ['shrill', 'loud', 'argumentative', 'irrational', 'angry', 'abusive', 'obnoxious', 'controlling', 'nagging', 'brash', 'hostile', 'mean', 'harsh', 'sassy', 'aggressive', 'opinionated', 'domineering'],\n",
    "        'competent': ['competent', 'productive', 'effective', 'ambitious', 'active', 'decisive', 'strong', 'tough'], 'incompetent': ['incompetent', 'unproductive', 'ineffective', 'passive', 'indecisive', 'weak', 'gentle', 'timid'],\n",
    "        'likeable': ['agreeable', 'fair', 'honest', 'trustworthy', 'selfless', 'accommodating'],\n",
    "        'unlikeable': ['abrasive', 'conniving', 'manipulative', 'dishonest', 'selfish', 'pushy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = sets.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to establish a common base of identity terminology of groups used to debias the word embedding, an equality set of words is built. The choice for equality sets stems from the fact that hard debiasing requires equality sets to debias. With the use of the same equality set in each method, a more meaningful comparison can be made. \n",
    "\n",
    "The equality set was built by hand, with some notions taken from Manzini, Lim, Tsvetkov and Black (2019) equality sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets['judaism_words'] = ['judaism', 'jew','jews','synagogue','synagogues', 'torah', 'rabbi', 'rabbis', \n",
    "                         'abraham', 'star', 'shabbat']\n",
    "sets['christianity_words'] = ['christianity','christian','christians','church','churches','bible','priest','priests',\n",
    "                             'jesus','cross','easter']\n",
    "sets['islam_words'] = ['islam','muslim','muslims','mosque','mosques','quran','imam','imams','muhammad',\n",
    "                        'hilal','ramadan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "equality_sets = [\n",
    "        [\"judaism\", \"christianity\", \"islam\"],\n",
    "        [\"jew\", \"christian\", \"muslim\"],\n",
    "        [\"jews\", \"christians\", \"muslims\"],\n",
    "        [\"synagogue\", \"church\", \"mosque\"],\n",
    "        [\"synagogues\", \"churches\", \"mosques\"],\n",
    "        [\"torah\", \"bible\", \"quran\"],\n",
    "        [\"rabbi\", \"priest\", \"imam\"],\n",
    "        [\"rabbis\", \"priests\", \"imams\"],\n",
    "        [\"abraham\", \"jesus\", \"muhammad\"],\n",
    "        [\"star\", \"cross\", \"hilal\"],\n",
    "        [\"shabbat\", \"easter\", \"ramadan\"],\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_p(nonDB_target_eval, target_eval):\n",
    "    '''\n",
    "    nonDB_target_eval: first distribution\n",
    "    target_eval: second distribution\n",
    "    returns p value of a one taile dt test between the two distributions\n",
    "    '''\n",
    "    diff = []\n",
    "    #differences between non debias and this method\n",
    "    for i in range(len(target_eval)):\n",
    "        diff.append(target_eval[i]- nonDB_target_eval[i])    \n",
    "    diff = np.array(diff)\n",
    "    std = np.std(diff)\n",
    "    mean = np.mean(diff)\n",
    "    st_error = std/(np.sqrt(len(diff)))\n",
    "    test_stat = mean/st_error\n",
    "    p_val = stats.t.sf(np.absolute(test_stat),df=len(diff)-1)\n",
    "    print('test stat', test_stat)\n",
    "    print('p ', p_val )\n",
    "    return p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Negative Sentiment Bias\n",
    "The first evaluation method will be the RNSB metric. It imitates the results, which would be given to a downstream application.\n",
    "The relative negative sentiment bias is an approach proposed by Sweeney and Najafan (2019) in order to offer insights on the effect of biased word embeddings through downstream applications. Its framework involves training a logistic classifier to predict the positive or negative sentiment of a given word. The classifier is trained on unbiased sentiment words, which are encoded via the word embedding to be investigated. Sweeney and Najafan then encode demographic identity terms and predict their respective negative sentiment probability. These results are used to form a probability distribution P. The Relative Negative Sentiment Bias (RNSB) is then defined \"as the KL\n",
    "divergence of P from U, where U is the uniform\n",
    "distribution\". \n",
    "\n",
    "The RNSB metric will be calculated for each debiasing method. To  intrepret this metric, Sweeney and Najafian state that \"Our RNSB metric captures the distance, via KL divergence, between the current distribution of negative sentiment and the fair uniform distribution. So the more fair a word embedding model with respect to sentiment bias, the lower the RNSB metric.\"\n",
    "\n",
    "*(Sweeney, C., & Najafian, M. (2019, July). A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (p. 1664).)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInReview(reviewName):\n",
    "    firstReview = open(reviewName, \"r\")\n",
    "    review = firstReview.read()\n",
    "    #Each list element is a word or title\n",
    "    reviewList = re.split(r'\\n', review)\n",
    "    #remove introduction to dataset\n",
    "    count = 0\n",
    "    for x in reviewList:\n",
    "        if x.startswith(';'):\n",
    "            count +=1\n",
    "        else:\n",
    "            break\n",
    "    reviewList = reviewList[count:]\n",
    "    return reviewList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(model_Encoding,review):\n",
    "    embedding_matrix = []\n",
    "    for x in review:\n",
    "        if x in model:\n",
    "            embedding_matrix.append(model_Encoding(x))\n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRNSB(results):\n",
    "    negProbResults = []\n",
    "    negProbSum = 0\n",
    "    for x in results:\n",
    "        negProbResults.append(x[0])\n",
    "        negProbSum += x[0]\n",
    "\n",
    "    normalizedResults = []\n",
    "    for i in range(len(results)): \n",
    "        normalizedResults.append(negProbResults[i]/negProbSum)\n",
    "        \n",
    "    #KL(P || Q) = sum x in X P(x) * log(P(x) / Q(x))\n",
    "    RNSB = 0\n",
    "    Q = 1/len(normalizedResults)\n",
    "    for x in normalizedResults:\n",
    "        RNSB += x * np.math.log((x / Q))\n",
    "    return RNSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNSB_Evaluation(model_Encoding,identity_words,p_val = True):\n",
    "    '''\n",
    "    model_encoding: methodcall to get a word embedding for a word\n",
    "    identity_words: equaluty sets to be tested\n",
    "    p_val: whether a one tailed t test on the distribution of the values should be performed\n",
    "    returns: RNSB score, p value and average RNSB scores for each equality set\n",
    "    \n",
    "    '''\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    #Train logistic classifier on unbiased positive /negative sentiment words\n",
    "    reviewNeg = readInReview(\"opinion-lexicon-English\\\\negative-words.txt\")\n",
    "    neg = create_embeddings(model_Encoding,reviewNeg)\n",
    "    targetN = np.zeros(np.size(neg,0))\n",
    "    reviewPos = readInReview(\"opinion-lexicon-English\\\\positive-words.txt\")\n",
    "    pos = create_embeddings(model_Encoding,reviewPos)\n",
    "    targetP = np.ones(np.size(pos,0))\n",
    "    #print('length of neg targets', len(targetN))\n",
    "    #print('length of pos targets', len(targetP))\n",
    "    embed = np.concatenate((pos,neg))\n",
    "    target = np.concatenate((targetP,targetN))\n",
    "    #Train, (Validation) and Test set \n",
    "    rnsb_values = []\n",
    "    stats = np.zeros(len(identity_words))\n",
    "    for n in range (0, 20):\n",
    "        itrain, itest = train_test_split(range(embed.shape[0]), test_size=0.2)\n",
    "\n",
    "        X_train = embed[itrain, :]\n",
    "        X_test = embed[itest, :]\n",
    "        y_train = target[itrain]\n",
    "        y_test = target[itest]\n",
    "\n",
    "\n",
    "        logisticReg = LogisticRegression()\n",
    "        logisticReg.fit(X_train,y_train)\n",
    "        logpredy = logisticReg.predict(X_test)\n",
    "        #print(\"Accuracy:\",metrics.accuracy_score(y_test, logpredy))\n",
    "        #print(\"Recall:\", metrics.recall_score(y_test, logpredy))\n",
    "        #print(\"Precision:\", metrics.precision_score(y_test, logpredy))\n",
    "        #print(\"F1: \", metrics.f1_score(y_test, logpredy))\n",
    "        RNSB = []\n",
    "        for iset in identity_words: \n",
    "            identity_embed= []\n",
    "            for i in iset:\n",
    "                if not model_Encoding(i) == 'None':\n",
    "                    identity_embed.append(model_Encoding(i))\n",
    "            identity_pred = logisticReg.predict_proba(identity_embed)\n",
    "            temp = calculateRNSB(identity_pred)\n",
    "            RNSB.append(temp)\n",
    "            #print('RNSB ', temp, 'of ', iset )\n",
    "        RNSB = np.array(RNSB)\n",
    "        rnsb_values.append(np.mean(RNSB))\n",
    "        stats = np.add(stats,RNSB)\n",
    "    \n",
    "    stats = np.divide(stats,20)\n",
    "    if p_val:\n",
    "        _,RNSBnonDB_target_eval,_ = RNSB_Evaluation(encode_nondebias,equality_sets,p_val = False)\n",
    "        p_val = ttest_p(RNSBnonDB_target_eval, stats)\n",
    "    else:\n",
    "        p_val = 0\n",
    "    print(p_val)\n",
    "    score = mean(rnsb_values)\n",
    "    print('average over 20 ', score)\n",
    "    return score, stats, p_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Cosine Similarity (MAC)\n",
    "The second evalutation method is the Mean Average Cosine Similarity (MAC) as proposed by Manzini et al. The method originates from the notion of the WEAT test implemented below, but allows for quantification of multiclass bias.\n",
    "\n",
    "The approach uses Target sets T, which are identity terms for a protected group, and thus inherently carry some bias, and attribute sets A containing terms that should not be linked to the target sets above (pleasent, unpleasent. The MAC metric is acquired by calculating the mean over the cosine distances between each element in a particular set T to each element in a particular set A. This is repeated for each set T with respect to each set A. The MAC is then the average result over all sets.\n",
    "\n",
    "The cosine distance ranges from 0 to 2, 0 indicating that the vectors are the same and 2 indicating that the vectors have maximum cosine distance. Thus, to interpret the MAC metric, one should ideally expect a MAC value of close to 1. This would indicate that the target sets are as close to an attribute set, as they are distant from it.\n",
    "\n",
    "*(Manzini, T., Lim, Y. C., Tsvetkov, Y., & Black, A. W. (2019). Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. arXiv preprint arXiv:1904.04047.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_evaluation(word_encoding, calculate_p_val = True):\n",
    "    '''\n",
    "    word_encoding : methodcall to get a word embedding for a word\n",
    "    calculate_p_val : whether a p value for the one tailed t test should be caluclated \n",
    "    returns MAC score, p value, individual MAC scores for each equality sets\n",
    "    '''\n",
    "    targets_eval = []\n",
    "    for targetSet in equality_sets:\n",
    "        for target in targetSet:\n",
    "            if word_encoding(target) != 'None':\n",
    "                for attributeSet in attributes:\n",
    "                    res = 0\n",
    "                    count = 0\n",
    "                    for a in attributeSet: \n",
    "                        if word_encoding(a)!= 'None':\n",
    "                            t_embed = word_encoding(target)\n",
    "                            a_embed = word_encoding(a)\n",
    "                            temp = cosine(t_embed,a_embed)\n",
    "                            res+= temp\n",
    "                            count += 1\n",
    "                            #print(f'{target} and {a} : {temp}')\n",
    "                            \n",
    "                    targets_eval.append((res/count))\n",
    "    if calculate_p_val: \n",
    "        _,_,nonDB_target_eval = multiclass_evaluation(encode_nondebias, False)\n",
    "        p_val = ttest_p(nonDB_target_eval,targets_eval)\n",
    "    else: \n",
    "        p_val = 0\n",
    "    m_score = np.mean(targets_eval)\n",
    "    return m_score, p_val, targets_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Evaluation Association Test (WEAT) Effect Size\n",
    "The WEAT also utilizes target and attribute sets as defined above. The WEAT considers two target and two attribute sets simultaeneously. It tests the association between one target and attribute set, relative to the association of the other target and attribute set. \"null hypothesis is that there is no difference between the two sets of target\n",
    "words in terms of their relative similarity to the two sets of attribute words\"\n",
    "\n",
    "\n",
    "To perform the WEAT test, the mean cosine similarity of an element in target set T1 to all elements in attribute set A2 is subtracted from the mean cosine similarity to all elements in attribute set A1. The sum of this is taken for all elements of T1. From this the sum for all elements in T2 is subtracted. This value is defined as the test statistic. \n",
    "\n",
    "To obtain the effect size, which is \"a normalized measure of how separated the two distributions (of associations between the target and attribute) are\" (Caliskan et al, 2017), the mean of the mean cosine similarity of each target of a target set T1 to each attribute set is taken. From this the mean of the mean cosine similarity of each target of a target set T2 to each attribute set is subtracted. To normalize this, this value is devided by the standard deviation over the mean cosine distance of each target in T1 and T2 to attribute set A1 minus that to attribute set A2. \n",
    "\n",
    "The loewer the effect size d is, i.e. the closer to zero, the less bias can be recorded between the target and attribute sets.\n",
    "\n",
    "\n",
    "*(Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion ={('islam_words', 'judaism_words'): [('likeable', 'unlikeable'), ('competent', 'incompetent'), \n",
    "                                                 ('shy', 'aggressive'), ('intellectual_words', 'appearance_words'),\n",
    "                                                 ('family', 'career'), ('instruments', 'weapons'), \n",
    "                                                 ('pleasant', 'unpleasant'), ('science', 'art')], \n",
    "           \n",
    "              ('islam_words', 'christianity_words'): [('likeable', 'unlikeable'), ('competent', 'incompetent'),\n",
    "                                                      ('shy', 'aggressive'), ('intellectual_words', 'appearance_words'),\n",
    "                                                      ('family', 'career'), ('instruments', 'weapons'), \n",
    "                                                      ('pleasant', 'unpleasant'), ('science', 'art')], \n",
    "           \n",
    "              ('judaism_words', 'christianity_words'): [('likeable', 'unlikeable'), ('competent', 'incompetent'), \n",
    "                                                        ('shy', 'aggressive'), ('intellectual_words', 'appearance_words'),\n",
    "                                                        ('family', 'career'), ('instruments', 'weapons'), \n",
    "                                                        ('pleasant', 'unpleasant'), ('science', 'art')]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WEAT_Effect_size(x,y,a,b):\n",
    "\n",
    "        numerator = np.mean([h(w,a,b) for w in x]) - np.mean([h(w,a,b) for w in y])\n",
    "        denominator = np.std([h(w,a,b) for w in x + y])\n",
    "        return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(w,A,B):\n",
    "    meanA = np.mean([cosine(w,a) for a in A])\n",
    "    meanB = np.mean([cosine(w,b) for b in B])\n",
    "    return (meanA - meanB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WEAT_Average_Effect_size(embedding_meth, biasToBeTested):\n",
    "    count = 0\n",
    "    d_values = []\n",
    "\n",
    "    for category_target_pair in biasToBeTested:\n",
    "        cat_0 = [embedding_meth(c) for c in sets[category_target_pair[0]]]\n",
    "        cat_1 = [embedding_meth(c) for c in sets[category_target_pair[1]]]\n",
    "        for attribute_pair in biasToBeTested[category_target_pair]:\n",
    "            att_0 = [embedding_meth(a) for a in sets[attribute_pair[0]]]\n",
    "            att_1 = [embedding_meth(a) for a in sets[attribute_pair[1]]]\n",
    "            d = WEAT_Effect_size(cat_0, cat_1, att_0, att_1)\n",
    "            d_values.append(np.abs(d))\n",
    "                \n",
    "    return np.mean(d_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Debiased Word Embedding Evaluation\n",
    "\n",
    "This is the Conceptnet model prior to any debiasing. It should perform the worst.\n",
    "http://blog.conceptnet.io/posts/2019/conceptnet-numberbatch-19-08/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_nondebias(word):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "average over 20  0.022338587200976912\n"
     ]
    }
   ],
   "source": [
    "relBias,RNSBnonDB_target_eval,p = RNSB_Evaluation(encode_nondebias,equality_sets, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test stat nan\n",
      "p  nan\n"
     ]
    }
   ],
   "source": [
    "score,p2,t = multiclass_evaluation(encode_nondebias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat = WEAT_Average_Effect_size(encode_nondebias, religion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results['Nondebias'] = [relBias, p, score,p2, weat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debiasing via Concept Negator\n",
    "\n",
    "Conceptor Debiasing (Karve, Ungar and Sedoc (2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNegatedConceptor(Z, aperture = 10):\n",
    "    '''\n",
    "    Find conceptor to debias with\n",
    "    '''\n",
    "    k = len(Z)\n",
    "    Z_Trans = Z.transpose()\n",
    "    C_1 = (1/k) * np.matmul(Z,Z_Trans)\n",
    "    temp = (aperture**(-2)) * np.identity(300)\n",
    "    C_2 = np.linalg.inv(np.add(C_1, temp))\n",
    "    C = np.matmul(C_1,C_2)\n",
    "    G = np.subtract(np.identity(300),C)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeViaConceptorNegator(word):\n",
    "    if word in model:\n",
    "        word = model[word]\n",
    "        word = word / np.linalg.norm(word)\n",
    "        return np.matmul(negConceptor,word) \n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Words \n",
    "Z = sets['judaism_words']+sets['christianity_words']+sets['islam_words']\n",
    "Z = list(set(Z))\n",
    "Z_embed = []\n",
    "for z in Z:\n",
    "    if z in model:\n",
    "        Z_embed.append(model[z])\n",
    "Z_embed = np.array(Z_embed)\n",
    "Z_embed  = Z_embed.transpose()\n",
    "negConceptor = findNegatedConceptor(Z_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "average over 20  0.02223597023922132\n",
      "test stat -2.1087088452750127\n",
      "p  0.03058744230543485\n",
      "0.03058744230543485\n",
      "average over 20  0.0077765203557548295\n"
     ]
    }
   ],
   "source": [
    "relBias,_,p = RNSB_Evaluation(encodeViaConceptorNegator,equality_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test stat 2.7129199637036367\n",
      "p  0.003443719362742512\n"
     ]
    }
   ],
   "source": [
    "score,p2,t = multiclass_evaluation(encodeViaConceptorNegator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat = WEAT_Average_Effect_size(encodeViaConceptorNegator, religion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results['Conceptor_Debias'] = [relBias, p,score,p2 ,weat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debiasing via Hard Debiasing\n",
    "\n",
    "Debiasing via Hard Debiasing\n",
    "Hard Debiasing Manzini et al. (2019) Adapted from :https://github.com/TManzini/DebiasMulticlassWordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the Bias Subspace\n",
    "# Taken and adapted from Mazini et als implementation\n",
    "\n",
    "def identify_bias_subspace(def_sets, subspace_dim, embedding_dim):\n",
    "    \"\"\"\n",
    "    Similar to bolukbasi's implementation at\n",
    "    https://github.com/tolga-b/debiaswe/blob/master/debiaswe/debias.py\n",
    "    vocab - dictionary mapping words to embeddings\n",
    "    def_sets - sets of words that represent extremes? of the subspace\n",
    "            we're interested in (e.g. man-woman, boy-girl, etc. for binary gender)\n",
    "    subspace_dim - number of vectors defining the subspace\n",
    "    embedding_dim - dimensions of the word embeddings\n",
    "    \"\"\"\n",
    "    # calculate means of defining sets # calculate vectors to perform PCA\n",
    "    means = {} \n",
    "    matrix = []\n",
    "    for term_list in def_sets:\n",
    "        wSet = []\n",
    "        for w in term_list:\n",
    "            if w in model: \n",
    "                wSet.append(model[w])\n",
    "\n",
    "        set_vectors = np.array(wSet)\n",
    "        means = np.mean(set_vectors, axis=0)\n",
    "        diffs = set_vectors - means\n",
    "        matrix.append(diffs)\n",
    "\n",
    "    matrix = np.concatenate(matrix)\n",
    "\n",
    "    pca = PCA(n_components=subspace_dim)\n",
    "    pca.fit(matrix)\n",
    "\n",
    "    return pca.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_subspace(vector, subspace):\n",
    "    v_b = np.zeros_like(vector)\n",
    "    for component in subspace:\n",
    "        v_b += np.dot(vector.transpose(), component) * component\n",
    "    return v_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeHardDebias(word): \n",
    "    # is the word in the model vocab\n",
    "    if word not in model: \n",
    "        return 'None'\n",
    "    wordEmbed = model[word]\n",
    "    \n",
    "    #if the word should contain bias, i.e. is found in following sets: \n",
    "    eq_sets = equality_sets\n",
    "    eq_set =[]\n",
    "    for i in eq_sets:\n",
    "        if word in i:\n",
    "            eq_set = i\n",
    "    # is the word in a set\n",
    "    if len(eq_set) != 0:\n",
    "        \n",
    "    #embed the set, and simultaeneuosly compute the mean\n",
    "        mean = np.zeros((300,))\n",
    "        cleanEqSet = []    \n",
    "        for w in eq_set:\n",
    "            if w in model and not w == word:\n",
    "                v = model[w]\n",
    "                v = v / np.linalg.norm(v)\n",
    "                mean += v\n",
    "                cleanEqSet.append(v)\n",
    "            elif w == word:\n",
    "                wordEmbed = wordEmbed/np.linalg.norm(wordEmbed)\n",
    "                mean += wordEmbed\n",
    "        mean /= float(len(cleanEqSet))\n",
    "        \n",
    "        #bias subspace component of the mean\n",
    "        mean_b = project_onto_subspace(mean, bias_subspace)\n",
    "        upsilon = mean - mean_b\n",
    "        v_b = project_onto_subspace(wordEmbed, bias_subspace)\n",
    "        frac = (v_b - mean_b) / np.linalg.norm(v_b - mean_b)\n",
    "        wordEmbed = upsilon + np.sqrt(1 - np.square(np.absolute(upsilon))) * frac\n",
    "        return wordEmbed\n",
    "    # if it should not contain any bias\n",
    "    v_b = project_onto_subspace(wordEmbed, bias_subspace)\n",
    "    wordEmbed = (wordEmbed - v_b) / np.linalg.norm(wordEmbed - v_b)\n",
    "\n",
    "    return wordEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definite sets and equality sets\n",
    "bias_subspace = identify_bias_subspace(equality_sets, 1, 300 )\n",
    "#make bias subsapce 2d\n",
    "if bias_subspace.ndim == 1:\n",
    "    bias_subspace = np.expand_dims(bias_subspace, 0)\n",
    "elif bias_subspace.ndim != 2:\n",
    "    raise ValueError(\"bias subspace should be either a matrix or vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "average over 20  0.02232367971646843\n",
      "test stat -2.2407341340713973\n",
      "p  0.024471621311983733\n",
      "0.024471621311983733\n",
      "average over 20  4.3134371900243536e-06\n"
     ]
    }
   ],
   "source": [
    "relBias,_,p = RNSB_Evaluation(encodeHardDebias,equality_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test stat -1.6277679618852339\n",
      "p  0.05208588234222636\n"
     ]
    }
   ],
   "source": [
    "score,p2,t = multiclass_evaluation(encodeHardDebias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat = WEAT_Average_Effect_size(encodeHardDebias, religion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results['Hard_Debias'] = [relBias, p, score, p2, weat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftWEAT\n",
    "\n",
    "Popovic et al. (2020) Adapted from: https://github.com/RadomirPopovicFON/Joint-Multiclass-Debiasing-of-Word-Embeddings/tree/9c15ba7299599ccee6170ac6ec0d25cde95a3778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_t = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WEAT(embedding_meth, biasToBeTested, threshold = thresh):\n",
    "    to_debias_dict = {'islam_words': [], 'christianity_words':[], 'judaism_words':[]}\n",
    "    d_values = []\n",
    "    for category_target_pair in biasToBeTested:\n",
    "        cat_0 = [embedding_meth(c) for c in sets[category_target_pair[0]]]\n",
    "        cat_1 = [embedding_meth(c) for c in sets[category_target_pair[1]]]\n",
    "        for attribute_pair in biasToBeTested[category_target_pair]:\n",
    "            att_0 = [embedding_meth(a) for a in sets[attribute_pair[0]]]\n",
    "            att_1 = [embedding_meth(a) for a in sets[attribute_pair[1]]]\n",
    "            d = WEAT_Effect_size(cat_0, cat_1, att_0, att_1)\n",
    "            print(f'{attribute_pair[0]} , {attribute_pair[1]}, {category_target_pair[0]}, {category_target_pair[1]} = {d}')\n",
    "            d_values.append(abs(d))\n",
    "            if np.abs(d) > threshold:\n",
    "                if d > 0:\n",
    "                    if attribute_pair[0] not in to_debias_dict[category_target_pair[0]]:\n",
    "                        to_debias_dict[category_target_pair[0]].append(attribute_pair[0])\n",
    "                    if attribute_pair[1] not in to_debias_dict[category_target_pair[1]]:\n",
    "                        to_debias_dict[category_target_pair[1]].append(attribute_pair[1])\n",
    "                else: \n",
    "                    if attribute_pair[1] not in to_debias_dict[category_target_pair[0]]:\n",
    "                        to_debias_dict[category_target_pair[0]].append(attribute_pair[1])\n",
    "                    if attribute_pair[0] not in to_debias_dict[category_target_pair[1]]:\n",
    "                        to_debias_dict[category_target_pair[1]].append(attribute_pair[0]) \n",
    "    return to_debias_dict, np.mean(d_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\n",
    "def find_closest_embeddings(embedding, n):\n",
    "    temp = sorted(model.keys(), key=lambda word: cosine(model[word], model[embedding]))\n",
    "    return temp[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word, w):\n",
    "    return cosine(model[word], model[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNeighbourWords(classes, n):\n",
    "    '''\n",
    "    classes: target sets for Christianity, Islam, Judaism \n",
    "    n: number of neighbours\n",
    "    returns n extra neighbours for each subclass\n",
    "    '''\n",
    "    ## Find neighbouring words\n",
    "    newWords = {}\n",
    "    for subclass in classes.keys(): \n",
    "        print('finding neighbouring words for ', subclass)\n",
    "        for word in classes[subclass]: \n",
    "            print('finding neighbouring words for ', word)\n",
    "            neighbours = find_closest_embeddings(word, n)\n",
    "            for neighbour in neighbours:\n",
    "                if neighbour not in newWords.keys():\n",
    "                    newWords[neighbour] = [subclass]\n",
    "                elif subclass not in newWords[neighbour]:\n",
    "                    newWords[neighbour].append(subclass)\n",
    "    print('Done finding words ....')\n",
    "    for word in list(newWords.keys()):\n",
    "        for subclasses in classes.keys():\n",
    "            if word in classes[subclasses]:\n",
    "                print(subclasses , ' ', word)\n",
    "                if word in newWords:\n",
    "                    del newWords[word]\n",
    "    for word, value in newWords.items():\n",
    "        if len(value) == 1:\n",
    "            classes[value[0]].append(word)\n",
    "        else: \n",
    "            print('duplicate : ', word, ' in ', value)\n",
    "            best = 0\n",
    "            bestClass = None\n",
    "            for v in value:\n",
    "                sum = 0\n",
    "                c = 0 \n",
    "                for w in classes[v]:\n",
    "                    c+=1\n",
    "                    sum += similarity(word,w)\n",
    "                mean = sum/c\n",
    "                if mean > best:\n",
    "                    best = mean\n",
    "                    bestClass = v\n",
    "            print('duplicate : ', word, ' in ', value, ' put in ', bestClass, ' with ', best)\n",
    "            classes[bestClass].append(word)\n",
    "    print('Neighbours')\n",
    "    for s in classes:\n",
    "        print(s, len(classes[s]))\n",
    "        print(classes[s])\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddingsoftWEAT(word, classes, T = final_t):\n",
    "    if word not in model: \n",
    "        return 'None'\n",
    "    for sc in classes.keys():\n",
    "        if word in classes[sc] and T[sc] != []:\n",
    "            t = T[sc]\n",
    "            word = np.array(model[word])\n",
    "            word = np.append(word, 1)\n",
    "            v =  np.dot(t, word)[0:-1]\n",
    "            return v / np.linalg.norm(v)\n",
    "    return model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WEATtest(embedding_meth, T, classes, biasToBeTested, threshold = thresh):\n",
    "    d_values = []\n",
    "    for category_target_pair in biasToBeTested:\n",
    "        cat_0 = [embedding_meth(c,classes,T) for c in sets[category_target_pair[0]]]\n",
    "        cat_1 = [embedding_meth(c,classes,T) for c in sets[category_target_pair[1]]]\n",
    "        for attribute_pair in biasToBeTested[category_target_pair]:\n",
    "            att_0 = [embedding_meth(a,classes,T) for a in sets[attribute_pair[0]]]\n",
    "            att_1 = [embedding_meth(a,classes,T) for a in sets[attribute_pair[1]]]\n",
    "            d = WEAT_Effect_size(cat_0, cat_1, att_0, att_1)\n",
    "            d_values.append(abs(d))\n",
    " \n",
    "    return np.mean(d_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getT(debias_dict, all_attribute_words, classes, l):\n",
    "    t = {'islam_words': [], 'christianity_words':[], 'judaism_words':[]}\n",
    "    final_t = {}\n",
    "    for s in debias_dict.keys():\n",
    "        biasedAttr = debias_dict[s]\n",
    "        print(biasedAttr, type(biasedAttr))\n",
    "        targetWords = classes[s] \n",
    "        for w in all_attribute_words:\n",
    "            if w in targetWords:\n",
    "                targetWords.remove(w)\n",
    "        targetWords = [model[w] for w in targetWords if w in model]\n",
    "        meanTargetWord = np.mean(targetWords, axis = 0)\n",
    "        meanAttSets = []\n",
    "        for attset in biasedAttr:\n",
    "            tempSet = [model[w] for w in sets[attset]]\n",
    "            meanAttSets.append(np.mean(tempSet, axis = 0))\n",
    "        meanMatrix = np.array(meanAttSets)\n",
    "        null_space_vectors = ns(meanMatrix)\n",
    "        temp = 100\n",
    "        d_vals = []\n",
    "        best = 1\n",
    "        bestMat = None\n",
    "        for n in range(temp):\n",
    "            T = null_space_vectors[:,n] - meanTargetWord\n",
    "            TI = np.identity(len(T)+1)\n",
    "            for t_i, value in enumerate(T):\n",
    "                TI[t_i, -1] = value * l \n",
    "            t[s] = TI\n",
    "            d = WEATtest(embeddingsoftWEAT,t,classes, religion)\n",
    "            #print('effect size d = ',d) \n",
    "            if d < best: \n",
    "                best = d\n",
    "                bestMat = TI\n",
    "            d_vals.append(d)\n",
    "        final_t[s] = TI\n",
    "    return final_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softWEAT(lamda):\n",
    "    #get attribute sets to debias to\n",
    "    debias_dict,_ = WEAT(encode_nondebias,religion)\n",
    "    print(debias_dict)\n",
    "    \n",
    "    #establish target words and neighbours to debias\n",
    "    classes = {}\n",
    "    classes['islam_words'] = sets['islam_words'].copy()\n",
    "    classes['christianity_words'] = sets['christianity_words'].copy()\n",
    "    classes['judaism_words'] = sets['judaism_words'].copy()\n",
    "    classes = addNeighbourWords(classes, 10)\n",
    "    for s in classes:\n",
    "        print(s, len(classes[s]))\n",
    "        \n",
    "    #list all words in each attribute set\n",
    "    all_attribute_words = []\n",
    "    for s in sets.keys():\n",
    "        if s != 'christianity_words' and s!= 'islam_words' and s!= 'judaism_words':\n",
    "            all_attribute_words.extend(sets[s])\n",
    "    print('All Attribute Words', all_attribute_words)\n",
    "    \n",
    "    final_t = {}\n",
    "    final_t = getT(debias_dict, all_attribute_words, classes,lamda)\n",
    "    return final_t, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likeable , unlikeable, islam_words, judaism_words = -0.9856263440964548\n",
      "competent , incompetent, islam_words, judaism_words = 1.3035998096564017\n",
      "shy , aggressive, islam_words, judaism_words = 0.6048398303482232\n",
      "intellectual_words , appearance_words, islam_words, judaism_words = 1.5246392743216168\n",
      "family , career, islam_words, judaism_words = 0.3561610325250264\n",
      "instruments , weapons, islam_words, judaism_words = 1.4271367595721247\n",
      "pleasant , unpleasant, islam_words, judaism_words = 0.1517951817140258\n",
      "science , art, islam_words, judaism_words = 0.3995852310902504\n",
      "likeable , unlikeable, islam_words, christianity_words = -0.5336664478564374\n",
      "competent , incompetent, islam_words, christianity_words = 1.421793005092937\n",
      "shy , aggressive, islam_words, christianity_words = 0.27497027985875794\n",
      "intellectual_words , appearance_words, islam_words, christianity_words = 0.9278409544133168\n",
      "family , career, islam_words, christianity_words = 0.2807237113609414\n",
      "instruments , weapons, islam_words, christianity_words = 0.8865447195837157\n",
      "pleasant , unpleasant, islam_words, christianity_words = 1.1161543279577129\n",
      "science , art, islam_words, christianity_words = -1.0291393997256024\n",
      "likeable , unlikeable, judaism_words, christianity_words = 0.7802469191898286\n",
      "competent , incompetent, judaism_words, christianity_words = -0.05579627995099751\n",
      "shy , aggressive, judaism_words, christianity_words = -0.3827581203815536\n",
      "intellectual_words , appearance_words, judaism_words, christianity_words = -0.8927912230446627\n",
      "family , career, judaism_words, christianity_words = -0.11420487779295094\n",
      "instruments , weapons, judaism_words, christianity_words = -0.6833329917521336\n",
      "pleasant , unpleasant, judaism_words, christianity_words = 1.1318690215815128\n",
      "science , art, judaism_words, christianity_words = -1.14622268392707\n",
      "{'islam_words': ['unlikeable', 'competent', 'shy', 'intellectual_words', 'instruments', 'pleasant', 'art'], 'christianity_words': ['likeable', 'incompetent', 'appearance_words', 'weapons', 'unpleasant', 'science', 'unlikeable', 'intellectual_words', 'instruments'], 'judaism_words': ['likeable', 'incompetent', 'aggressive', 'appearance_words', 'weapons', 'pleasant', 'art']}\n",
      "finding neighbouring words for  islam_words\n",
      "finding neighbouring words for  islam\n",
      "finding neighbouring words for  muslim\n",
      "finding neighbouring words for  muslims\n",
      "finding neighbouring words for  mosque\n",
      "finding neighbouring words for  mosques\n",
      "finding neighbouring words for  quran\n",
      "finding neighbouring words for  imam\n",
      "finding neighbouring words for  imams\n",
      "finding neighbouring words for  muhammad\n",
      "finding neighbouring words for  hilal\n",
      "finding neighbouring words for  ramadan\n",
      "finding neighbouring words for  christianity_words\n",
      "finding neighbouring words for  christianity\n",
      "finding neighbouring words for  christian\n",
      "finding neighbouring words for  christians\n",
      "finding neighbouring words for  church\n",
      "finding neighbouring words for  churches\n",
      "finding neighbouring words for  bible\n",
      "finding neighbouring words for  priest\n",
      "finding neighbouring words for  priests\n",
      "finding neighbouring words for  jesus\n",
      "finding neighbouring words for  cross\n",
      "finding neighbouring words for  easter\n",
      "finding neighbouring words for  judaism_words\n",
      "finding neighbouring words for  judaism\n",
      "finding neighbouring words for  jew\n",
      "finding neighbouring words for  jews\n",
      "finding neighbouring words for  synagogue\n",
      "finding neighbouring words for  synagogues\n",
      "finding neighbouring words for  torah\n",
      "finding neighbouring words for  rabbi\n",
      "finding neighbouring words for  rabbis\n",
      "finding neighbouring words for  abraham\n",
      "finding neighbouring words for  star\n",
      "finding neighbouring words for  shabbat\n",
      "Done finding words ....\n",
      "islam_words   islam\n",
      "islam_words   muslim\n",
      "islam_words   muslims\n",
      "islam_words   mosque\n",
      "islam_words   mosques\n",
      "islam_words   quran\n",
      "islam_words   imam\n",
      "islam_words   imams\n",
      "islam_words   muhammad\n",
      "islam_words   hilal\n",
      "islam_words   ramadan\n",
      "christianity_words   christianity\n",
      "christianity_words   christian\n",
      "christianity_words   christians\n",
      "christianity_words   church\n",
      "christianity_words   churches\n",
      "christianity_words   bible\n",
      "christianity_words   priest\n",
      "christianity_words   priests\n",
      "christianity_words   jesus\n",
      "christianity_words   cross\n",
      "christianity_words   easter\n",
      "judaism_words   judaism\n",
      "judaism_words   jew\n",
      "judaism_words   jews\n",
      "judaism_words   synagogue\n",
      "judaism_words   synagogues\n",
      "judaism_words   torah\n",
      "judaism_words   rabbi\n",
      "judaism_words   rabbis\n",
      "judaism_words   abraham\n",
      "judaism_words   star\n",
      "judaism_words   shabbat\n",
      "Neighbours\n",
      "islam_words 91\n",
      "['islam', 'muslim', 'muslims', 'mosque', 'mosques', 'quran', 'imam', 'imams', 'muhammad', 'hilal', 'ramadan', 'islamness', 'night_of_power', 'murjite', 'muslimism', 'aga_khanite', 'muhammadanism', 'bakriyyah', 'moslemism', 'islamology', 'cyber_muslim', 'muslimite', 'muslimless', 'philo_muslim', 'post_muslim', 'promuslim', 'muslimdom', 'muslimship', 'muslimhood', 'muslimo', 'semi_muslim', 'un_muslim', 'muslimist', 'pre_muslim', 'muslimness', 'muslimville', 'mosqued', 'antimosque', 'mosque_goer', 'masjid', 'supermosque', 'mosque_going', 'megamosque', 'mosqueing', 'victory_mosque', 'quranize', 'quranite', 'quranist', 'quranologist', 'koranite', 'qur', 'koranize', 'ahle_quran', 'koran', 'imaum', 'i_have', 'imamate', 'dikka', 'iman', 'kutub_al_sittah', 'muslim_person', 'mullah', 'sheikhs', 'ulema', 'sahih_muslim', 'elijah_muhammad', 'seal_of_prophets', 'muhammadism', 'manticratic', 'muhammadian', 'mohammad', 'mohammed', 'mahomet', 'muhammadist', 'hijri', 'rabi_al_awwal', 'rabi_al_thani', 'islamic_month', 'jumada_al_awwal', 'sha_ban', 'islamic_calendar_month', 'islamic_calendar', 'dhu_l_hijja', 'laylat_al_qadr', 'islamic_holy_day', 'eid_al_fitr', 'laylatul_qadr', 'sugar_feast', 'eid_al_saghir', 'feast_of_fasting', 'lesser_eid']\n",
      "christianity_words 89\n",
      "['christianity', 'christian', 'christians', 'church', 'churches', 'bible', 'priest', 'priests', 'jesus', 'cross', 'easter', 'unchristianize', 'apocalyptic_number', 'komboskini', 'lampadarios', 'apocatastatic', 'apostolic_age', 'christianities', 'adoptianist', 'antiprotestant', 'christianess', 'christianish', 'paleochristian', 'christiany', 'christianlike', 'dechristianization', 'even_christian', 'unitarian_universalists', 'dechristianize', 'christophobe', 'christianophobe', 'church_militant', 'christlessness', 'antichurch', 'word_of_faith', 'churchful', 'churchship', 'dischurch', \"god's_house\", 'kingdom_hall', 'church_central_passage', 'worship_house', 'antiecclesiastical', 'christian_bible', 'holy_writ', 'extrabiblical', 'bibles', 'biblist', 'good_book', 'prayer_of_manasseh', 'counterbiblical', 'in_saecula_saeculorum', 'orthodox_priest', 'priesting', 'priestery', 'underpriest', 'unpriest', 'priestlike', 'confess_sins', 'domestic_prelate', 'priestliness', 'antisacerdotal', 'good_man', 'good_teacher', 'yeshua', 'jesuses', 'jesusanity', 'jesuslike', 'prince_of_peace', 'jesus_of_nazareth', 'christ', 'calvary_cross', 'papal_cross', 'crucigerous', 'lorraine_cross', 'crosseth', 'crosst', 'jerusalem_cross', 'crosslike', 'crosses', 'happy_easter', 'eastertime', 'cheaster', 'colored_eggs', 'pascha', 'easter_day', 'pasch', 'rogation_sunday', 'movable_feast']\n",
      "judaism_words 88\n",
      "['judaism', 'jew', 'jews', 'synagogue', 'synagogues', 'torah', 'rabbi', 'rabbis', 'abraham', 'star', 'shabbat', 'fast_of_esther', 'fast_of_firstborn', 'fast_of_gedaliah', 'fast_of_tammuz', 'fast_of_tevet', 'reform_judaism', 'conservative_judaism', 'balabos', 'high_holidays', 'ex_jew', 'jewless', 'judeophilic', 'pro_jew', 'yahudi', 'judeophobic', 'philo_semitic', 'jewishy', 'jewish', 'isreal', 'philo_semitism', 'synagogical', 'synagogal', 'synagog', 'jewish_people', 'synagogue_goer', 'synagogue_going', 'synagoguegoer', 'shul', 'gabbai', 'cybersynagogue', 'beth_hamidrash', 'pentateuchal', 'leyn', 'nevi_im', 'tikkun_leil_shavuot', 'hebrew_scriptures', 'pentateuch', 'setumah', 'tanach', 'rav', 'amora', 'carlebachian', 'rebbetzin', 'rebbe', 'rebbetzman', 'rabbinate', 'rabbinic', 'rabbinical', 'rabbinically', 'kolpik', 'abrahamical', 'abrahams', 'abram', 'gaybraham', 'abrahamize', 'abrahamitic', 'abie', 'ibrahim', 'abrahamist', 'nonstar', 'blue_hook_star', 'late_type_star', 'epsilon_aurigae', 'early_type_star', 'optical_double', 'starward', 'astriferous', 'blue_giant_star', 'shabbos', 'sabbatise', 'shabbos_goy', 'sabbatize', 'sabbath', 'sabbathless', 'sabbathly', 'sabbatharian', 'havdalah']\n",
      "islam_words 91\n",
      "christianity_words 89\n",
      "judaism_words 88\n",
      "All Attribute Words ['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise', 'family', 'happy', 'laughter', 'paradise', 'vacation', 'joy', 'wonderful', 'abuse', 'crash', 'filth', 'murder', 'sickness', 'accident', 'death', 'grief', 'poison', 'stink', 'assault', 'disaster', 'hatred', 'pollute', 'tragedy', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'kill', 'rotten', 'vomit', 'agony', 'prison', 'terrible', 'horrible', 'bagpipe', 'cello', 'guitar', 'lute', 'trombone', 'banjo', 'clarinet', 'harmonica', 'mandolin', 'trumpet', 'bassoon', 'drum', 'harp', 'oboe', 'tuba', 'bell', 'fiddle', 'harpsichord', 'piano', 'viola', 'bongo', 'flute', 'horn', 'saxophone', 'arrow', 'club', 'gun', 'missile', 'spear', 'dagger', 'harpoon', 'pistol', 'sword', 'blade', 'dynamite', 'hatchet', 'rifle', 'tank', 'bomb', 'firearm', 'knife', 'shotgun', 'teargas', 'cannon', 'grenade', 'mace', 'slingshot', 'whip', 'executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career', 'home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives', 'science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment', 'astronomy', 'poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture', 'resourceful', 'inquisitive', 'sagacious', 'inventive', 'adaptable', 'intuitive', 'analytical', 'imaginative', 'shrewd', 'thoughtful', 'smart', 'clever', 'brilliant', 'logical', 'intelligent', 'apt', 'genius', 'wise', 'stupid', 'dumb', 'dull', 'clumsy', 'foolish', 'naive', 'unintelligent', 'alluring', 'voluptuous', 'blushing', 'homely', 'plump', 'sensual', 'gorgeous', 'slim', 'bald', 'athletic', 'fashionable', 'stout', 'ugly', 'muscular', 'slender', 'feeble', 'handsome', 'healthy', 'attractive', 'fat', 'weak', 'thin', 'pretty', 'beautiful', 'strong', 'soft', 'quiet', 'compromising', 'rational', 'calm', 'kind', 'agreeable', 'servile', 'pleasant', 'cautious', 'friendly', 'supportive', 'nice', 'mild', 'demure', 'passive', 'indifferent', 'shrill', 'loud', 'argumentative', 'irrational', 'angry', 'abusive', 'obnoxious', 'controlling', 'nagging', 'brash', 'hostile', 'mean', 'harsh', 'sassy', 'aggressive', 'opinionated', 'domineering', 'competent', 'productive', 'effective', 'ambitious', 'active', 'decisive', 'strong', 'tough', 'incompetent', 'unproductive', 'ineffective', 'passive', 'indecisive', 'weak', 'gentle', 'timid', 'agreeable', 'fair', 'honest', 'trustworthy', 'selfless', 'accommodating', 'abrasive', 'conniving', 'manipulative', 'dishonest', 'selfish', 'pushy']\n",
      "['unlikeable', 'competent', 'shy', 'intellectual_words', 'instruments', 'pleasant', 'art'] <class 'list'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['likeable', 'incompetent', 'appearance_words', 'weapons', 'unpleasant', 'science', 'unlikeable', 'intellectual_words', 'instruments'] <class 'list'>\n",
      "['likeable', 'incompetent', 'aggressive', 'appearance_words', 'weapons', 'pleasant', 'art'] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.5\n",
    "final_t, classes = softWEAT(lamda= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddingsoftWEATEval(word):\n",
    "    T = final_t\n",
    "    if word not in model: \n",
    "        return 'None'\n",
    "    for sc in classes.keys():\n",
    "        if word in classes[sc]:\n",
    "            if T[sc] != []:\n",
    "                t = T[sc]\n",
    "                word = np.array(model[word])\n",
    "                word = np.append(word, 1)\n",
    "                v =  np.dot(t, word)[0:-1]\n",
    "                return v / np.linalg.norm(v)\n",
    "    v = model[word]\n",
    "    return v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "average over 20  0.023061897370802456\n",
      "test stat -1.7036920753719718\n",
      "p  0.05963003348674883\n",
      "0.05963003348674883\n",
      "average over 20  0.018435919137171325\n",
      "test stat 33.173205066853114\n",
      "p  1.9426450067693869e-131\n"
     ]
    }
   ],
   "source": [
    "relBias,_,p  = RNSB_Evaluation(embeddingsoftWEATEval,equality_sets)\n",
    "score,p2,t = multiclass_evaluation(embeddingsoftWEATEval)\n",
    "weat = WEAT_Average_Effect_size(embeddingsoftWEATEval, religion)\n",
    "Results['softWEAT_Debias'] = [relBias,p, score, p2,weat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nondebias\n",
      "RNSB Score: 0.02234 with a p value of  0;\n",
      "MAC Score: 0.99519; with a p value of nan\n",
      "WEAT Score: 0.76714 \n",
      "Conceptor_Debias\n",
      "RNSB Score: 0.00778 with a p value of  0.03059;\n",
      "MAC Score: 0.99695; with a p value of 0.00344\n",
      "WEAT Score: 0.30251 \n",
      "Hard_Debias\n",
      "RNSB Score: 0.0 with a p value of  0.02447;\n",
      "MAC Score: 0.99376; with a p value of 0.05209\n",
      "WEAT Score: 0.00441 \n",
      "softWEAT_Debias\n",
      "RNSB Score: 0.01844 with a p value of  0.05963;\n",
      "MAC Score: 1.01367; with a p value of 0.0\n",
      "WEAT Score: 0.83589 \n"
     ]
    }
   ],
   "source": [
    "for key, value in Results.items():\n",
    "    print(key)\n",
    "    print(f'RNSB Score: {round(value[0],5)} with a p value of  {round(value[1],5)};')\n",
    "    print(f'MAC Score: {round(value[2],5)}; with a p value of {round(value[3],5)}')      \n",
    "    print(f'WEAT Score: {round(value[4],5)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nondebias : 0.022338587200976912\n",
      " Conceptor_Debias : 0.0077765203557548295\n",
      " Hard_Debias : 4.3134371900243536e-06\n",
      " softWEAT_Debias : 0.018435919137171325\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RNSB = [d[0] for d in Results.values()]\n",
    "for key, value in Results.items():\n",
    "    print(f' {key} : {value[0]}')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.title('RNSB on Judaism, Christianity, Islam')\n",
    "plt.bar(Results.keys(), RNSB)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nondebias : 0\n",
      " Conceptor_Debias : 0.03058744230543485\n",
      " Hard_Debias : 0.024471621311983733\n",
      " softWEAT_Debias : 0.05963003348674883\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAC = [d[2] for d in Results.values()]\n",
    "for key, value in Results.items():\n",
    "    print(f' {key} : {value[1]}')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.title('Mean Average Cosine Similarity')\n",
    "plt.bar(Results.keys(), MAC)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nondebias : 0.9951864821818667\n",
      " Conceptor_Debias : 0.9969530022943499\n",
      " Hard_Debias : 0.993761308398741\n",
      " softWEAT_Debias : 1.0136715176252593\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WEATD = [d[4] for d in Results.values()]\n",
    "for key, value in Results.items():\n",
    "    print(f' {key} : {value[2]}')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.title('WEAT Effect size')\n",
    "plt.bar(Results.keys(), WEATD)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Thesis Code)",
   "language": "python",
   "name": "pycharm-8c054aba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
