{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0Ga0Ndxe4J7"
   },
   "source": [
    "# \"Thy algorithm shalt not bear false witness\": An Evaluation of Multiclass Debiasing Methods on Word Embeddings\n",
    "\n",
    "\n",
    "This is the code to support the paper \"\"Thy algorithm shalt not bear false witness\": An Evaluation of Multiclass Debiasing Methods on Word Embeddings\"\n",
    " It shows, which experiments and debiasing techniques are applied to word embedding word2vec.\n",
    "The approaches are namely: \n",
    "\n",
    "Debiasing approaches: \n",
    "*   Conceptor Debiasing\n",
    "*   Hard Debiasing\n",
    "*   softWEAT\n",
    "\n",
    "Evaluation Metrics: \n",
    "*   Relative Negative Sentiment Bias(RNSB) Metric\n",
    "*   Mean Average Cosine Similarity (MAC) Metric\n",
    "*   Word Association Evaluation Test (WEAT) effect size Metric\n",
    "\n",
    "This Notebook refers to the W2V embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7HibcAUe4J8"
   },
   "outputs": [],
   "source": [
    "#Import needed Packages\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import null_space as ns\n",
    "from statistics import mean\n",
    "from scipy.linalg import norm\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "import seaborn as sns;\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xI38K6M7e4KL"
   },
   "source": [
    "# Loading Word2Vec\n",
    "The \"pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\" (https://code.google.com/archive/p/word2vec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6stczDfRe4KL"
   },
   "outputs": [],
   "source": [
    "#for work in juypter notebooks\n",
    "model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors//GoogleNews-vectors-negative300.bin\", binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Below the data for the debaising and evaluation methods are defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sets of words concerning a particular attribute class are taken from Popovic, Lemmerich and Strohmaier (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "377FxSuFkkr5"
   },
   "outputs": [],
   "source": [
    "sets = {'pleasant': ['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise', 'family', 'happy', 'laughter', 'paradise', 'vacation', 'joy', 'wonderful'],\n",
    "        'unpleasant': ['abuse', 'crash', 'filth', 'murder', 'sickness', 'accident', 'death', 'grief', 'poison', 'stink', 'assault', 'disaster', 'hatred', 'pollute', 'tragedy', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'kill', 'rotten', 'vomit', 'agony', 'prison', 'terrible', 'horrible'],\n",
    "        'instruments': ['bagpipe', 'cello', 'guitar', 'lute', 'trombone', 'banjo', 'clarinet', 'harmonica', 'mandolin', 'trumpet', 'bassoon', 'drum', 'harp', 'oboe', 'tuba', 'bell', 'fiddle', 'harpsichord', 'piano', 'viola', 'bongo', 'flute', 'horn', 'saxophone'],\n",
    "        'weapons': ['arrow', 'club', 'gun', 'missile', 'spear', 'dagger', 'harpoon', 'pistol', 'sword', 'blade', 'dynamite', 'hatchet', 'rifle', 'tank', 'bomb', 'firearm', 'knife', 'shotgun', 'teargas', 'cannon', 'grenade', 'mace', 'slingshot', 'whip'],\n",
    "        'career': ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career'],\n",
    "        'family': ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives'],\n",
    "        'science': ['science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment', 'astronomy'],\n",
    "        'art': ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture'], \n",
    "        'intellectual_words': ['resourceful', 'inquisitive', 'sagacious', 'inventive', 'adaptable', 'intuitive', 'analytical', 'imaginative', 'shrewd', 'thoughtful', 'smart', 'clever', 'brilliant', 'logical', 'intelligent', 'apt', 'genius', 'wise', 'stupid', 'dumb', 'dull', 'clumsy', 'foolish', 'naive', 'unintelligent'],\n",
    "        'appearance_words': ['alluring', 'voluptuous', 'blushing', 'homely', 'plump', 'sensual', 'gorgeous', 'slim', 'bald', 'athletic', 'fashionable', 'stout', 'ugly', 'muscular', 'slender', 'feeble', 'handsome', 'healthy', 'attractive', 'fat', 'weak', 'thin', 'pretty', 'beautiful', 'strong'], \n",
    "        'shy': ['soft', 'quiet', 'compromising', 'rational', 'calm', 'kind', 'agreeable', 'servile', 'pleasant', 'cautious', 'friendly', 'supportive', 'nice', 'mild', 'demure', 'passive', 'indifferent'],\n",
    "        'aggressive': ['shrill', 'loud', 'argumentative', 'irrational', 'angry', 'abusive', 'obnoxious', 'controlling', 'nagging', 'brash', 'hostile', 'mean', 'harsh', 'sassy', 'aggressive', 'opinionated', 'domineering'],\n",
    "        'competent': ['competent', 'productive', 'effective', 'ambitious', 'active', 'decisive', 'strong', 'tough'], 'incompetent': ['incompetent', 'unproductive', 'ineffective', 'passive', 'indecisive', 'weak', 'gentle', 'timid'],\n",
    "        'likeable': ['agreeable', 'fair', 'honest', 'trustworthy', 'selfless', 'accommodating'],\n",
    "        'unlikeable': ['abrasive', 'conniving', 'manipulative', 'dishonest', 'selfish', 'pushy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = sets.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to establish a common base of identity terminology of groups used to debias the word embedding, an equality set of words is built. The choice for equality sets stems from the fact that hard debiasing requires equality sets to debias. With the use of the same equality set in each method, a more meaningful comparison can be made. \n",
    "\n",
    "The equality set was built by hand, with some notions taken from Manzini, Lim, Tsvetkov and Black (2019) equality sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets['judaism_words'] = ['judaism', 'jew','jews','synagogue','synagogues', 'torah', 'rabbi', 'rabbis', \n",
    "                         'abraham', 'star', 'shabbat']\n",
    "sets['christianity_words'] = ['christianity','christian','christians','church','churches','bible','priest','priests',\n",
    "                             'jesus','cross','easter']\n",
    "sets['islam_words'] = ['islam','muslim','muslims','mosque','mosques','quran','imam','imams','muhammad',\n",
    "                        'hilal','ramadan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "equality_sets = [\n",
    "        [\"judaism\", \"christianity\", \"islam\"],\n",
    "        [\"jew\", \"christian\", \"muslim\"],\n",
    "        [\"jews\", \"christians\", \"muslims\"],\n",
    "        [\"synagogue\", \"church\", \"mosque\"],\n",
    "        [\"synagogues\", \"churches\", \"mosques\"],\n",
    "        [\"torah\", \"bible\", \"quran\"],\n",
    "        [\"rabbi\", \"priest\", \"imam\"],\n",
    "        [\"rabbis\", \"priests\", \"imams\"],\n",
    "        [\"abraham\", \"jesus\", \"muhammad\"],\n",
    "        [\"star\", \"cross\", \"hilal\"],\n",
    "        [\"shabbat\", \"easter\", \"ramadan\"],\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing of MAC for Religion specific average cosine  distance to attribute sets. This was done for the analysis of the \n",
    "#poor performance of hard debiasing within MAC \n",
    "mequality_sets = [\n",
    "    ['judaism', 'jew','jews','synagogue','synagogues', 'torah', 'rabbi', 'rabbis', \n",
    "                         'abraham', 'star', 'shabbat'],\n",
    "    ['christianity','christian','christians','church','churches','bible','priest','priests',\n",
    "                             'jesus','cross','easter'],\n",
    "    ['islam','muslim','muslims','mosque','mosques','quran','imam','imams','muhammad',\n",
    "                        'hilal','ramadan']\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5YuqPqTe4KO"
   },
   "source": [
    "# Bias Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYxib3nOyfF1"
   },
   "outputs": [],
   "source": [
    "Results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CBslGgawe4KP"
   },
   "source": [
    "## Relative Negative Sentiment Bias (RNSB)\n",
    "The first evaluation method will be the RNSB metric. It imitates the results, which would be given to a downstream application.\n",
    "The relative negative sentiment bias is an approach proposed by Sweeney and Najafan (2019) in order to offer insights on the effect of biased word embeddings through downstream applications. Its framework involves training a logistic classifier to predict the positive or negative sentiment of a given word. The classifier is trained on unbiased sentiment words, which are encoded via the word embedding to be investigated. Sweeney and Najafan then encode demographic identity terms and predict their respective negative sentiment probability. These results are used to form a probability distribution P. The Relative Negative Sentiment Bias (RNSB) is then defined \"as the KL\n",
    "divergence of P from U, where U is the uniform\n",
    "distribution\". \n",
    "\n",
    "The RNSB metric will be calculated for each debiasing method. To  intrepret this metric, Sweeney and Najafian state that \"Our RNSB metric captures the distance, via KL divergence, between the current distribution of negative sentiment and the fair uniform distribution. So the more fair a word embedding model with respect to sentiment bias, the lower the RNSB metric.\"\n",
    "\n",
    "*(Sweeney, C., & Najafian, M. (2019, July). A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (p. 1664).)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3qWVze3e4KP"
   },
   "outputs": [],
   "source": [
    "def readInReview(reviewName):\n",
    "    firstReview = open(reviewName, \"r\")\n",
    "    review = firstReview.read()\n",
    "    #Each list element is a word or title\n",
    "    reviewList = re.split(r'\\n', review)\n",
    "    #remove introduction to dataset\n",
    "    count = 0\n",
    "    for x in reviewList:\n",
    "        if x.startswith(';'):\n",
    "            count +=1\n",
    "        else:\n",
    "            break\n",
    "    reviewList = reviewList[count:]\n",
    "    return reviewList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xALbK-poe4KZ"
   },
   "outputs": [],
   "source": [
    "def create_embeddings(model_Encoding,review):\n",
    "    embedding_matrix = []\n",
    "    for x in review:\n",
    "        if x in model.vocab:\n",
    "            embedding_matrix.append(model_Encoding(x))\n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oFP4t6d5e4Kc"
   },
   "outputs": [],
   "source": [
    "def calculateRNSB(results):\n",
    "    negProbResults = []\n",
    "    negProbSum = 0\n",
    "    for x in results:\n",
    "        negProbResults.append(x[0])\n",
    "        negProbSum += x[0]\n",
    "\n",
    "    normalizedResults = []\n",
    "    for i in range(len(results)): \n",
    "        normalizedResults.append(negProbResults[i]/negProbSum)\n",
    "        \n",
    "    #KL Divergence\n",
    "    RNSB = 0\n",
    "    Q = 1/len(normalizedResults)\n",
    "    for x in normalizedResults:\n",
    "        RNSB += x * np.math.log((x / Q))\n",
    "    return RNSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-iDCm4ke4Ke"
   },
   "outputs": [],
   "source": [
    "def RNSB_Evaluation(model_Encoding,identity_words,p_val = True):\n",
    "    '''\n",
    "    model_encoding: methodcall to get a word embedding for a word\n",
    "    identity_words: equaluty sets to be tested\n",
    "    p_val: whether a one tailed t test on the distribution of the values should be performed\n",
    "    returns: RNSB score, p value and average RNSB scores for each equality set\n",
    "    \n",
    "    '''\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    #Train logistic classifier on unbiased positive /negative sentiment words\n",
    "    reviewNeg = readInReview(\"opinion-lexicon-English\\\\negative-words.txt\")\n",
    "    neg = create_embeddings(model_Encoding,reviewNeg)\n",
    "    targetN = np.zeros(np.size(neg,0))\n",
    "    reviewPos = readInReview(\"opinion-lexicon-English\\\\positive-words.txt\")\n",
    "    pos = create_embeddings(model_Encoding,reviewPos)\n",
    "    targetP = np.ones(np.size(pos,0))\n",
    "    embed = np.concatenate((pos,neg))\n",
    "    target = np.concatenate((targetP,targetN))\n",
    "    \n",
    "    #Train, (Validation) and Test set \n",
    "    rnsb_values = []\n",
    "    stats = np.zeros(len(identity_words))\n",
    "    islam_val, christ_val, juda_val = [], [],[]\n",
    "    # for 20 runs\n",
    "    for n in range (0, 20):\n",
    "        itrain, itest = train_test_split(range(embed.shape[0]), test_size=0.2)\n",
    "\n",
    "        X_train = embed[itrain, :]\n",
    "        X_test = embed[itest, :]\n",
    "        y_train = target[itrain]\n",
    "        y_test = target[itest]\n",
    "\n",
    "\n",
    "        logisticReg = LogisticRegression()\n",
    "        logisticReg.fit(X_train,y_train)\n",
    "        logpredy = logisticReg.predict(X_test)\n",
    "        \n",
    "        #print(\"Accuracy:\",metrics.accuracy_score(y_test, logpredy))\n",
    "        #print(\"Recall:\", metrics.recall_score(y_test, logpredy))\n",
    "        #print(\"Precision:\", metrics.precision_score(y_test, logpredy))\n",
    "        #print(\"F1: \", metrics.f1_score(y_test, logpredy))\n",
    "        \n",
    "        RNSB = []\n",
    "        juda, christ, islam = [],[],[]\n",
    "        for iset in identity_words: \n",
    "            identity_embed= []\n",
    "            for i in iset:\n",
    "                if not model_Encoding(i) == 'None':\n",
    "                    identity_embed.append(model_Encoding(i))\n",
    "            identity_pred = logisticReg.predict_proba(identity_embed)\n",
    "            juda.append(identity_pred[0][0])\n",
    "            christ.append(identity_pred[1][0])\n",
    "            islam.append(identity_pred[2][0]) \n",
    "            temp = calculateRNSB(identity_pred)\n",
    "            RNSB.append(temp)\n",
    "                \n",
    "        juda_val.append(mean(juda))\n",
    "        christ_val.append(mean(christ))\n",
    "        islam_val.append(mean(islam)) \n",
    "        RNSB = np.array(RNSB)\n",
    "        rnsb_values.append(np.mean(RNSB))\n",
    "        stats = np.add(stats,RNSB)\n",
    "    j = mean(juda_val)\n",
    "    c = mean(christ_val)\n",
    "    i = mean(islam_val)\n",
    "    #averafe relative negative sentiment for each religion over various equality sets\n",
    "    print ('judaism ', j, 'christ', c, ' islam', i)\n",
    "    stats = np.divide(stats,20)\n",
    "    if p_val:\n",
    "        _,RNSBnonDB_target_eval,_ = RNSB_Evaluation(encode_nondebias,equality_sets,p_val = False)\n",
    "        p_val = ttest_p(RNSBnonDB_target_eval, stats)\n",
    "    else:\n",
    "        p_val = 0\n",
    "\n",
    "    score = mean(rnsb_values)\n",
    "    print('average over 20 ', score, stats)\n",
    "    return score, stats, p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GniwlqaTe4Ki"
   },
   "source": [
    "## Mean Average Cosine Similarity (MAC)\n",
    "The second evalutation method is the Mean Average Cosine Similarity (MAC) as proposed by Manzini et al. The method originates from the notion of the WEAT test implemented below, but allows for quantification of multiclass bias.\n",
    "\n",
    "The approach uses Target sets T, which are identity terms for a protected group, and thus inherently carry some bias, and attribute sets A containing terms that should not be linked to the target sets above (pleasent, unpleasent). The MAC metric is acquired by calculating the mean over the cosine distances between each element in a particular set T to each element in a particular set A. This is repeated for each set T with respect to each set A. The MAC is then the average result over all sets.\n",
    "\n",
    "The cosine distance ranges from 0 to 2, 0 indicating that the vectors are the same and 2 indicating that the vectors have maximum cosine distance. Thus, to interpret the MAC metric, one should ideally expect a MAC value of close to 1. This would indicate that the target sets are as close to an attribute set, as they are distant from it.\n",
    "\n",
    "*(Manzini, T., Lim, Y. C., Tsvetkov, Y., & Black, A. W. (2019). Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. arXiv preprint arXiv:1904.04047.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_y_6HETe4Kr"
   },
   "outputs": [],
   "source": [
    "def multiclass_evaluation(word_encoding, calculate_p_val = True):\n",
    "    '''\n",
    "    word_encoding : methodcall to get a word embedding for a word\n",
    "    calculate_p_val : whether a p value for the one tailed t test should be caluclated \n",
    "    returns MAC score, p value, individual MAC scores for each equality sets\n",
    "    '''\n",
    "    targets_eval = []\n",
    "    \n",
    "    for targetSet in equality_sets:\n",
    "        inestigatTemp = []\n",
    "        for target in targetSet:\n",
    "            if word_encoding(target) != 'None':\n",
    "                for attributeSet in attributes:\n",
    "                    res = 0\n",
    "                    count = 0\n",
    "                    for a in attributeSet: \n",
    "                        if word_encoding(a)!= 'None':\n",
    "                            t_embed = word_encoding(target)\n",
    "                            a_embed = word_encoding(a)\n",
    "                            #print(t_embed.dtype, a_embed.dtype)\n",
    "                            temp = cosine(t_embed,a_embed)\n",
    "                            res+= temp\n",
    "                            count += 1\n",
    "                            #print(f'{target} and {a} : {temp}')\n",
    "                    #print(target, attributeSet,(res/count) )\n",
    "                    targets_eval.append((res/count))\n",
    "                    inestigatTemp.append((res/count))\n",
    "        print(targetSet, ':', np.mean(inestigatTemp))\n",
    "    if calculate_p_val: \n",
    "        _,_,nonDB_target_eval = multiclass_evaluation(encode_nondebias, False)\n",
    "        p_val = ttest_p(nonDB_target_eval,targets_eval)\n",
    "    else: \n",
    "        p_val = 0\n",
    "    print('score:', np.sum(targets_eval)/(len(targets_eval)))\n",
    "    m_score = np.mean(targets_eval)\n",
    "    return m_score, p_val, targets_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_p(nonDB_target_eval, target_eval):\n",
    "    '''\n",
    "    nonDB_target_eval: first distribution\n",
    "    target_eval: second distribution\n",
    "    returns p value of a one taile dt test between the two distributions\n",
    "    '''\n",
    "    diff = []\n",
    "    #differences between non debias and this method\n",
    "    for i in range(len(target_eval)):\n",
    "        diff.append(target_eval[i]- nonDB_target_eval[i])    \n",
    "    diff = np.array(diff)\n",
    "    std = np.std(diff)\n",
    "    mean = np.mean(diff)\n",
    "    st_error = std/(np.sqrt(len(diff)))\n",
    "    test_stat = mean/st_error\n",
    "    p_val = stats.t.sf(np.absolute(test_stat),df=len(diff)-1)\n",
    "    print('test stat', test_stat)\n",
    "    print('p ', p_val )\n",
    "    return p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_GncRJQkaJB"
   },
   "source": [
    "## Word Evaluation Association Test (WEAT) Effect Size\n",
    "The WEAT also utilizes target and attribute sets as defined above. The WEAT considers two target and two attribute sets simultaeneously. It tests the association between one target and attribute set, relative to the association of the other target and attribute set. \"null hypothesis is that there is no difference between the two sets of target\n",
    "words in terms of their relative similarity to the two sets of attribute words\"\n",
    "\n",
    "\n",
    "To perform the WEAT test, the mean cosine similarity of an element in target set T1 to all elements in attribute set A2 is subtracted from the mean cosine similarity to all elements in attribute set A1. The sum of this is taken for all elements of T1. From this the sum for all elements in T2 is subtracted. This value is defined as the test statistic. \n",
    "\n",
    "To obtain the effect size, which is \"a normalized measure of how separated the two distributions (of associations between the target and attribute) are\" (Caliskan et al, 2017), the mean of the mean cosine similarity of each target of a target set T1 to each attribute set is taken. From this the mean of the mean cosine similarity of each target of a target set T2 to each attribute set is subtracted. To normalize this, this value is devided by the standard deviation over the mean cosine distance of each target in T1 and T2 to attribute set A1 minus that to attribute set A2. \n",
    "\n",
    "The loewer the effect size d is, i.e. the closer to zero, the less bias can be recorded between the target and attribute sets.\n",
    "\n",
    "\n",
    "*(Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'religion' dictionary defines logical combinations for attribute sets. Each of these is defined for each combination of target sets. (Sets againare taken from Popovic et al.'s work (2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZvwnaY68kkr1"
   },
   "outputs": [],
   "source": [
    "religion ={('islam_words', 'judaism_words'): [('likeable', 'unlikeable'), ('competent', 'incompetent'), \n",
    "                                                 ('shy', 'aggressive'), ('intellectual_words', 'appearance_words'),\n",
    "                                                 ('family', 'career'), ('instruments', 'weapons'), \n",
    "                                                 ('pleasant', 'unpleasant'), ('science', 'art')], \n",
    "           \n",
    "              ('islam_words', 'christianity_words'): [('likeable', 'unlikeable'), ('competent', 'incompetent'),\n",
    "                                                      ('shy', 'aggressive'), ('intellectual_words', 'appearance_words'),\n",
    "                                                      ('family', 'career'), ('instruments', 'weapons'), \n",
    "                                                      ('pleasant', 'unpleasant'), ('science', 'art')], \n",
    "           \n",
    "              ('judaism_words', 'christianity_words'): [('likeable', 'unlikeable'), ('competent', 'incompetent'), \n",
    "                                                        ('shy', 'aggressive'), ('intellectual_words', 'appearance_words'),\n",
    "                                                        ('family', 'career'), ('instruments', 'weapons'), \n",
    "                                                        ('pleasant', 'unpleasant'), ('science', 'art')]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5pUgy4fkksK"
   },
   "outputs": [],
   "source": [
    "def WEAT_Effect_size(x,y,a,b):\n",
    "        numerator = np.mean([h(w,a,b) for w in x]) - np.mean([h(w,a,b) for w in y])\n",
    "        denominator = np.std([h(w,a,b) for w in x + y])\n",
    "        return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WEAT_p_val(X,Y,A,B):\n",
    "    '''\n",
    "    p value: probability that the statistic is of the value by chance\n",
    "    '''\n",
    "    stat = np.sum([h(w,A,B) for w in X]) -  np.sum([h(w,A,B) for w in Y])\n",
    "    print('stat', stat)\n",
    "    count = 0\n",
    "    higher = 0\n",
    "    #create all possible combinations\n",
    "    Z = X+Y\n",
    "    print(len(Z), len(list(combinations(range(len(Z)),len(Z)//2))))\n",
    "    combos = []\n",
    "    for combo in combinations(range(len(Z)), len(Z)//2):\n",
    "        combos.append(combo)\n",
    "        x = [Z[i] for i in combo]\n",
    "        y = []\n",
    "        for i in range(len(Z)):\n",
    "            if i not in combo:\n",
    "                y.append(Z[i])\n",
    "        if(x not in combos and y not in combos):\n",
    "            # calculate test statistic\n",
    "            stat_temp = np.sum([h(w,A,B) for w in x]) -  np.sum([h(w,A,B) for w in y])\n",
    "            count += 1\n",
    "            if stat_temp > stat:\n",
    "                higher += 1\n",
    "        #print(count)\n",
    "    p_val = higher/count \n",
    "    print('p_val', p_val)\n",
    "    return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jq04-YFckksM"
   },
   "outputs": [],
   "source": [
    "def h(w,A,B):\n",
    "    meanA = np.mean([cosine(w,a) for a in A])\n",
    "    meanB = np.mean([cosine(w,b) for b in B])\n",
    "    return (meanA - meanB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-JAYEnbkksO"
   },
   "outputs": [],
   "source": [
    "def WEAT_Average_Effect_size(embedding_meth, biasToBeTested, p = False):\n",
    "    '''\n",
    "    embedding_meth: method call to get the word vector of a word\n",
    "    biasToBeTested: attribute target set combinations (dict) \n",
    "    returns effect size d\n",
    "    '''\n",
    "    count = 0\n",
    "    d_values = []\n",
    "    p_values = []\n",
    "    for category_target_pair in biasToBeTested:\n",
    "        cat_0 = [embedding_meth(c) for c in sets[category_target_pair[0]]]\n",
    "        cat_1 = [embedding_meth(c) for c in sets[category_target_pair[1]]]\n",
    "        for attribute_pair in biasToBeTested[category_target_pair]:\n",
    "            att_0 = [embedding_meth(a) for a in sets[attribute_pair[0]]]\n",
    "            att_1 = [embedding_meth(a) for a in sets[attribute_pair[1]]]\n",
    "            if p : \n",
    "                p_values.append(WEAT_p_val(cat_0,cat_1,att_0,att_1))\n",
    "            d = WEAT_Effect_size(cat_0, cat_1, att_0, att_1)\n",
    "            d_values.append(abs(d))          \n",
    "    print(d_values) \n",
    "    return np.mean(d_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e7pP0Wfxe4K1"
   },
   "source": [
    "# Non-Debiased Word Embedding Evaluation\n",
    "\n",
    "This is the word2vec model prior to any debiasing. It should perform the worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYBH6Nm2e4K2"
   },
   "outputs": [],
   "source": [
    "def encode_nondebias(word):\n",
    "    if word in model.vocab:\n",
    "        return model.get_vector(word)\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judaism  0.5589326351558177 christ 0.6612342754523184  islam 0.8562495688071126\n",
      "average over 20  0.12823775556463923 [0.00277317 0.05016296 0.00449369 0.01115461 0.0146937  0.55661426\n",
      " 0.19707681 0.05558263 0.03022849 0.11165298 0.376182  ]\n"
     ]
    }
   ],
   "source": [
    "relBias,RNSBnonDB_target_eval,p = RNSB_Evaluation(encode_nondebias,equality_sets, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "X5M2_CaZe4LG",
    "outputId": "20ab5101-8bf3-4c59-c728-993119c2b4f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['judaism', 'christianity', 'islam'] : 0.7798762041004368\n",
      "['jew', 'christian', 'muslim'] : 0.7779697438448442\n",
      "['jews', 'christians', 'muslims'] : 0.8117313206891456\n",
      "['synagogue', 'church', 'mosque'] : 0.9913475976506388\n",
      "['synagogues', 'churches', 'mosques'] : 0.9812809082575095\n",
      "['torah', 'bible', 'quran'] : 0.8595037080193201\n",
      "['rabbi', 'priest', 'imam'] : 0.9981623588407634\n",
      "['rabbis', 'priests', 'imams'] : 0.9899504226712583\n",
      "['abraham', 'jesus', 'muhammad'] : 0.7449998714863378\n",
      "['star', 'cross', 'hilal'] : 0.9370946496251209\n",
      "['shabbat', 'easter', 'ramadan'] : 0.8314679134777648\n",
      "['judaism', 'christianity', 'islam'] : 0.7798762041004368\n",
      "['jew', 'christian', 'muslim'] : 0.7779697438448442\n",
      "['jews', 'christians', 'muslims'] : 0.8117313206891456\n",
      "['synagogue', 'church', 'mosque'] : 0.9913475976506388\n",
      "['synagogues', 'churches', 'mosques'] : 0.9812809082575095\n",
      "['torah', 'bible', 'quran'] : 0.8595037080193201\n",
      "['rabbi', 'priest', 'imam'] : 0.9981623588407634\n",
      "['rabbis', 'priests', 'imams'] : 0.9899504226712583\n",
      "['abraham', 'jesus', 'muhammad'] : 0.7449998714863378\n",
      "['star', 'cross', 'hilal'] : 0.9370946496251209\n",
      "['shabbat', 'easter', 'ramadan'] : 0.8314679134777648\n",
      "score: 0.8821258816966492\n",
      "test stat nan\n",
      "p  nan\n",
      "score: 0.8821258816966492\n"
     ]
    }
   ],
   "source": [
    "score,p2,t = multiclass_evaluation(encode_nondebias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J08VGmC2yfHB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26066891607626996, 0.40838164801997817, 0.005682144845930048, 0.4421097319788695, 0.4786866281807168, 1.2101514554789525, 0.9100249979993442, 0.41257207784203753, 0.28454531118961734, 0.7620611940494734, 0.030062573246757235, 0.2772419557116415, 0.30306180801277566, 0.7599421809029535, 1.0536442159511894, 0.31651217215973515, 0.018352683771000064, 0.2567078594016816, 0.023672850382630804, 0.17404635457394793, 0.15471786708628507, 0.8087072262644193, 0.041342490092856066, 0.07971854132974603]\n"
     ]
    }
   ],
   "source": [
    "Results['Nondebias*'] = [relBias, p, score,p2, WEAT_Average_Effect_size(encode_nondebias, religion)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czJHIIb6e4LN"
   },
   "source": [
    "## Debiasing via Concept Negator\n",
    "Conceptor Debiasing (Karve, Ungar and Sedoc (2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hr4DLrUCe4LO"
   },
   "outputs": [],
   "source": [
    "def findNegatedConceptor(Z, aperture = 10):\n",
    "    '''\n",
    "    Find conceptor to debias with\n",
    "    '''\n",
    "    k = len(Z)\n",
    "    Z_Trans = Z.transpose()\n",
    "    C_1 = (1/k) * np.matmul(Z,Z_Trans)\n",
    "    temp = (aperture**(-2)) * np.identity(300)\n",
    "    C_2 = np.linalg.inv(np.add(C_1, temp))\n",
    "    C = np.matmul(C_1,C_2)\n",
    "    G = np.subtract(np.identity(300),C)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-a9Aksmce4LV"
   },
   "outputs": [],
   "source": [
    "def encodeViaConceptorNegator(word):\n",
    "    if word in model.vocab:\n",
    "        word = model.get_vector(word)\n",
    "        word = word / np.linalg.norm(word)\n",
    "        return np.matmul(negConceptor,word) \n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzeP7w5Ae4La",
    "outputId": "e5de1eaf-1940-4b07-dcb7-afb36da607d0"
   },
   "outputs": [],
   "source": [
    "#Target Words \n",
    "Z = sets['judaism_words']+sets['christianity_words']+sets['islam_words']\n",
    "Z = list(set(Z))\n",
    "Z_embed = []\n",
    "for z in Z:\n",
    "    #print(z)\n",
    "    if z in model.vocab:\n",
    "        Z_embed.append(model.get_vector(z))\n",
    "Z_embed = np.array(Z_embed)\n",
    "Z_embed  = Z_embed.transpose()\n",
    "negConceptor = findNegatedConceptor(Z_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "ZNvPCRvTe4Lc",
    "outputId": "414a655f-52ad-48c7-8952-2b362c8bf057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judaism  0.6297078201253618 christ 0.63394208964055  islam 0.6085346236678507\n",
      "judaism  0.5764138341963406 christ 0.6742609959123592  islam 0.8587263033808307\n",
      "average over 20  0.11773116501125165 [0.0030145  0.04375814 0.00331389 0.00625541 0.01622483 0.50234391\n",
      " 0.1746793  0.06673468 0.03250383 0.10634404 0.33987031]\n",
      "test stat -2.4076965237993293\n",
      "p  0.018414037101185162\n",
      "average over 20  0.0067377115121234 [0.00098419 0.00788754 0.00417143 0.00062324 0.00100111 0.00631655\n",
      " 0.00380504 0.00231741 0.01060807 0.0181849  0.01821535]\n"
     ]
    }
   ],
   "source": [
    "relBias,_,p = RNSB_Evaluation(encodeViaConceptorNegator,equality_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "128bes4ae4Lh",
    "outputId": "36c7f893-0b33-4d90-85ee-44e2eedf5397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['judaism', 'christianity', 'islam'] : 0.9900593387721193\n",
      "['jew', 'christian', 'muslim'] : 0.9921138288368686\n",
      "['jews', 'christians', 'muslims'] : 1.0136340693732602\n",
      "['synagogue', 'church', 'mosque'] : 0.999470531876395\n",
      "['synagogues', 'churches', 'mosques'] : 0.9971036075651948\n",
      "['torah', 'bible', 'quran'] : 0.9991911233595253\n",
      "['rabbi', 'priest', 'imam'] : 1.0147167268321569\n",
      "['rabbis', 'priests', 'imams'] : 0.9960338245917869\n",
      "['abraham', 'jesus', 'muhammad'] : 0.9655393370766925\n",
      "['star', 'cross', 'hilal'] : 0.9963705877581445\n",
      "['shabbat', 'easter', 'ramadan'] : 0.9877862741683305\n",
      "['judaism', 'christianity', 'islam'] : 0.7798762041004368\n",
      "['jew', 'christian', 'muslim'] : 0.7779697438448442\n",
      "['jews', 'christians', 'muslims'] : 0.8117313206891456\n",
      "['synagogue', 'church', 'mosque'] : 0.9913475976506388\n",
      "['synagogues', 'churches', 'mosques'] : 0.9812809082575095\n",
      "['torah', 'bible', 'quran'] : 0.8595037080193201\n",
      "['rabbi', 'priest', 'imam'] : 0.9981623588407634\n",
      "['rabbis', 'priests', 'imams'] : 0.9899504226712583\n",
      "['abraham', 'jesus', 'muhammad'] : 0.7449998714863378\n",
      "['star', 'cross', 'hilal'] : 0.9370946496251209\n",
      "['shabbat', 'easter', 'ramadan'] : 0.8314679134777648\n",
      "score: 0.8821258816966492\n",
      "test stat 27.368519799539566\n",
      "p  1.4359195545117307e-103\n",
      "score: 0.9956381136554976\n"
     ]
    }
   ],
   "source": [
    "score,p2,t = multiclass_evaluation(encodeViaConceptorNegator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgMjECNIyfHn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29812402016897077, 0.4563255952120053, 0.0705083078797554, 0.003929635285323497, 0.21810506347895486, 0.4834117625812987, 0.07644292033070908, 0.011928756117349483, 0.33797939753890416, 0.02897332034258196, 0.12636178173207893, 0.11911219921971636, 0.2117372292740326, 0.0047950677532390285, 0.07100262342086357, 0.22709179326729018, 0.08142424591290265, 0.3815649752241952, 0.06642440284748305, 0.09120378605877584, 0.04931580876661513, 0.43287610235017365, 0.011927060075353945, 0.24638146496735128]\n"
     ]
    }
   ],
   "source": [
    "Results['Conceptor_Debias*'] = [relBias, p,score,p2 ,WEAT_Average_Effect_size(encodeViaConceptorNegator, religion)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zTXrSxGpe4Lq"
   },
   "source": [
    "## Debiasing via Hard Debiasing\n",
    "Hard Debiaing Manzini et al. (2019)\n",
    "Adapted from :https://github.com/TManzini/DebiasMulticlassWordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTlsGXDse4Ls"
   },
   "outputs": [],
   "source": [
    "def identify_bias_subspace(def_sets, subspace_dim, embedding_dim):\n",
    "    \"\"\"\n",
    "    Similar to bolukbasi's implementation at\n",
    "    https://github.com/tolga-b/debiaswe/blob/master/debiaswe/debias.py\n",
    "    vocab - dictionary mapping words to embeddings\n",
    "    def_sets - sets of words that represent extremes? of the subspace\n",
    "            we're interested in (e.g. man-woman, boy-girl, etc. for binary gender)\n",
    "    subspace_dim - number of vectors defining the subspace\n",
    "    embedding_dim - dimensions of the word embeddings\n",
    "    \"\"\"\n",
    "    # calculate means of defining sets # calculate vectors to perform PCA\n",
    "    means = {} \n",
    "    matrix = []\n",
    "    for term_list in def_sets:\n",
    "        wSet = []\n",
    "        for w in term_list:\n",
    "            if w in model.vocab: \n",
    "                wSet.append(model.get_vector(w))\n",
    "\n",
    "        set_vectors = np.array(wSet)\n",
    "        means = np.mean(set_vectors, axis=0)\n",
    "        diffs = set_vectors - means\n",
    "        matrix.append(diffs)\n",
    "\n",
    "    matrix = np.concatenate(matrix)\n",
    "\n",
    "    pca = PCA(n_components=subspace_dim)\n",
    "    pca.fit(matrix)\n",
    "\n",
    "    return pca.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vnj7SybRe4Lx"
   },
   "outputs": [],
   "source": [
    "def project_onto_subspace(vector, subspace):\n",
    "    '''\n",
    "    returns v_b : projection of the vector onto the subspace\n",
    "    '''\n",
    "    v_b = np.zeros_like(vector)\n",
    "    for component in subspace:\n",
    "        v_b += np.dot(vector.transpose(), component) * component\n",
    "    return v_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNkS0Q8Te4L0"
   },
   "outputs": [],
   "source": [
    "def encodeHardDebias(word): \n",
    "    # is the word in the model vocab\n",
    "    if word not in model.vocab: \n",
    "        return 'None'\n",
    "    wordEmbed = model.get_vector(word)\n",
    "    #if the word should contain bias, i.e. is found in following sets: \n",
    "    eq_sets = equality_sets\n",
    "    eq_set =[]\n",
    "    for i in eq_sets:\n",
    "        if word in i:\n",
    "            eq_set = i\n",
    "    # is the word in a set\n",
    "    if len(eq_set) != 0:\n",
    "    #embed the set, and simultaeneuosly compute the mean\n",
    "        mean = np.zeros((300,))\n",
    "        cleanEqSet = []    \n",
    "        for w in eq_set:\n",
    "            if w in model.vocab and not w == word:\n",
    "                v = model.get_vector(w)\n",
    "                v = v / np.linalg.norm(v)\n",
    "                mean += v\n",
    "                cleanEqSet.append(v)\n",
    "            elif w == word:\n",
    "                wordEmbed = wordEmbed/np.linalg.norm(wordEmbed)\n",
    "                mean += wordEmbed\n",
    "        mean /= float(len(cleanEqSet))\n",
    "        #bias subspace component of the mean\n",
    "        mean_b = project_onto_subspace(mean, bias_subspace)\n",
    "        upsilon = mean - mean_b\n",
    "        v_b = project_onto_subspace(wordEmbed, bias_subspace)\n",
    "        frac = (v_b - mean_b) / np.linalg.norm(v_b - mean_b)\n",
    "        wordEmbed = upsilon + np.sqrt(1 - np.square(np.absolute(upsilon))) * frac\n",
    "        return wordEmbed\n",
    "    # if it should not contain any bias\n",
    "    v_b = project_onto_subspace(wordEmbed, bias_subspace)\n",
    "    wordEmbed = (wordEmbed - v_b) / np.linalg.norm(wordEmbed - v_b)\n",
    "\n",
    "    return wordEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCTjtaaOe4L2"
   },
   "outputs": [],
   "source": [
    "#definite sets and equality sets\n",
    "bias_subspace = identify_bias_subspace(equality_sets, 1, 300 )\n",
    "#make bias subsapce 2d\n",
    "if bias_subspace.ndim == 1:\n",
    "    bias_subspace = np.expand_dims(bias_subspace, 0)\n",
    "elif bias_subspace.ndim != 2:\n",
    "    raise ValueError(\"bias subspace should be either a matrix or vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "7_qS0H4Ce4L5",
    "outputId": "f9ddaa69-5e40-43a2-f4ac-4661a761b861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judaism  0.6965450544858384 christ 0.6965450633807678  islam 0.696740432442311\n",
      "judaism  0.5815842325531919 christ 0.6730430284905986  islam 0.8533007079948625\n",
      "average over 20  0.11832997081623726 [0.00252362 0.04699935 0.00374248 0.00514197 0.01675147 0.52527554\n",
      " 0.15681514 0.05861417 0.03401117 0.08832383 0.36343094]\n",
      "test stat -2.4026285100834492\n",
      "p  0.01857421168741203\n",
      "average over 20  6.99909438110394e-07 [4.25335610e-08 6.30823048e-10 7.93835940e-09 2.66438703e-08\n",
      " 4.29301194e-07 8.65088726e-07 1.60063827e-07 3.94313537e-09\n",
      " 1.47101427e-07 3.90407830e-07 5.62535107e-06]\n"
     ]
    }
   ],
   "source": [
    "relBias,_,p = RNSB_Evaluation(encodeHardDebias,equality_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "1Gb868Ute4ME",
    "outputId": "e86ac89f-8c21-4d5e-ff0f-4ea55141289d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['judaism', 'christianity', 'islam'] : 0.796114061487387\n",
      "['jew', 'christian', 'muslim'] : 0.7933538217254936\n",
      "['jews', 'christians', 'muslims'] : 0.831459205123604\n",
      "['synagogue', 'church', 'mosque'] : 0.9956580725852627\n",
      "['synagogues', 'churches', 'mosques'] : 0.987419025139226\n",
      "['torah', 'bible', 'quran'] : 0.8647106121166969\n",
      "['rabbi', 'priest', 'imam'] : 1.0018412329511364\n",
      "['rabbis', 'priests', 'imams'] : 0.992961741304827\n",
      "['abraham', 'jesus', 'muhammad'] : 0.7561409402938336\n",
      "['star', 'cross', 'hilal'] : 0.9309662720577769\n",
      "['shabbat', 'easter', 'ramadan'] : 0.8350959474186453\n",
      "['judaism', 'christianity', 'islam'] : 0.7798762041004368\n",
      "['jew', 'christian', 'muslim'] : 0.7779697438448442\n",
      "['jews', 'christians', 'muslims'] : 0.8117313206891456\n",
      "['synagogue', 'church', 'mosque'] : 0.9913475976506388\n",
      "['synagogues', 'churches', 'mosques'] : 0.9812809082575095\n",
      "['torah', 'bible', 'quran'] : 0.8595037080193201\n",
      "['rabbi', 'priest', 'imam'] : 0.9981623588407634\n",
      "['rabbis', 'priests', 'imams'] : 0.9899504226712583\n",
      "['abraham', 'jesus', 'muhammad'] : 0.7449998714863378\n",
      "['star', 'cross', 'hilal'] : 0.9370946496251209\n",
      "['shabbat', 'easter', 'ramadan'] : 0.8314679134777648\n",
      "score: 0.8821258816966492\n",
      "test stat 5.425568234397417\n",
      "p  4.410203498887944e-08\n",
      "score: 0.8896109938367172\n"
     ]
    }
   ],
   "source": [
    "score,p2,t = multiclass_evaluation(encodeHardDebias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a50Qy75WyfIS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0009733277324354114, 0.0007559321745605632, 0.0019865040350492916, 0.0006642955537903662, 0.0013152318724061315, 0.0010726589650163912, 0.002813830962049836, 0.0003373649102302703, 0.0009733074794783077, 0.0007559924634458543, 0.001986392300647191, 0.0006643827094685886, 0.00131526024676284, 0.0010725830445126967, 0.002813831685660661, 0.0003373547038268864, 2.025648292462404e-08, 6.027450406422862e-08, 1.1192807905574676e-07, 8.715661893440438e-08, 2.8401712007243322e-08, 7.583482391263508e-08, 7.041712408395439e-10, 1.0212855665628998e-08]\n"
     ]
    }
   ],
   "source": [
    "Results['Hard_Debias*'] = [relBias, p, score, p2, WEAT_Average_Effect_size(encodeHardDebias, religion)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftWEAT\n",
    "Popovic et al. (2020)\n",
    "Adapted from: https://github.com/RadomirPopovicFON/Joint-Multiclass-Debiasing-of-Word-Embeddings/tree/9c15ba7299599ccee6170ac6ec0d25cde95a3778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_t = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get target dict\n",
    "def WEAT(embedding_meth, biasToBeTested, threshold = thresh):\n",
    "    to_debias_dict = {'islam_words': [], 'christianity_words':[], 'judaism_words':[]}\n",
    "    d_values = []\n",
    "    for category_target_pair in biasToBeTested:\n",
    "        cat_0 = [embedding_meth(c) for c in sets[category_target_pair[0]]]\n",
    "        cat_1 = [embedding_meth(c) for c in sets[category_target_pair[1]]]\n",
    "        for attribute_pair in biasToBeTested[category_target_pair]:\n",
    "            att_0 = [embedding_meth(a) for a in sets[attribute_pair[0]]]\n",
    "            att_1 = [embedding_meth(a) for a in sets[attribute_pair[1]]]\n",
    "            d = WEAT_Effect_size(cat_0, cat_1, att_0, att_1)\n",
    "            print(f'{attribute_pair[0]} , {attribute_pair[1]}, {category_target_pair[0]}, {category_target_pair[1]} = {d}')\n",
    "            d_values.append(abs(d))\n",
    "            if np.abs(d) > threshold:\n",
    "                if d > 0:\n",
    "                    if attribute_pair[0] not in to_debias_dict[category_target_pair[0]]:\n",
    "                        to_debias_dict[category_target_pair[0]].append(attribute_pair[0])\n",
    "                    if attribute_pair[1] not in to_debias_dict[category_target_pair[1]]:\n",
    "                        to_debias_dict[category_target_pair[1]].append(attribute_pair[1])\n",
    "                else: \n",
    "                    if attribute_pair[1] not in to_debias_dict[category_target_pair[0]]:\n",
    "                        to_debias_dict[category_target_pair[0]].append(attribute_pair[1])\n",
    "                    if attribute_pair[0] not in to_debias_dict[category_target_pair[1]]:\n",
    "                        to_debias_dict[category_target_pair[1]].append(attribute_pair[0]) \n",
    "    return to_debias_dict, np.mean(d_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNeighbourWords(classes, n):\n",
    "    '''\n",
    "    classes: target sets for Christianity, Islam, Judaism \n",
    "    n: number of neighbours\n",
    "    returns n extra neighbours for each subclass\n",
    "    '''\n",
    "    ## Find neighbouring words\n",
    "    newWords = {}\n",
    "    for subclass in classes.keys(): \n",
    "        for word in classes[subclass]: \n",
    "            neighbours = model.similar_by_word(word, topn=n)\n",
    "            for neighbour in neighbours:\n",
    "                if neighbour[1] > 0.5:\n",
    "                    if neighbour[0] not in newWords.keys():\n",
    "                        newWords[neighbour[0]] = [subclass]\n",
    "                    elif subclass not in newWords[neighbour[0]]:\n",
    "                        newWords[neighbour[0]].append(subclass)\n",
    "    \n",
    "    for word in list(newWords.keys()):\n",
    "        for subclasses in classes.keys():\n",
    "            if word in classes[subclasses]:\n",
    "                del newWords[word]\n",
    "    for word, value in newWords.items():\n",
    "        if len(value) == 1:\n",
    "            classes[value[0]].append(word)\n",
    "        else: \n",
    "            #print('duplicate : ', word, ' in ', value)\n",
    "            best = 0\n",
    "            bestClass = None\n",
    "            for v in value:\n",
    "                sum = 0\n",
    "                c = 0 \n",
    "                for w in classes[v]:\n",
    "                    c+=1\n",
    "                    sum += model.similarity(word,w)\n",
    "                mean = sum/c\n",
    "                if mean > best:\n",
    "                    best = mean\n",
    "                    bestClass = v\n",
    "            #print('duplicate : ', word, ' in ', value, ' put in ', bestClass, ' with ', best)\n",
    "            classes[bestClass].append(word)\n",
    "    for s in classes:\n",
    "        print(s, len(classes[s]))\n",
    "        print(classes[s])\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddingsoftWEAT(word, classes, T = final_t):\n",
    "    if word not in model.vocab: \n",
    "        return 'None'\n",
    "    for sc in classes.keys():\n",
    "        if word in classes[sc] and T[sc] != []:\n",
    "            t = T[sc]\n",
    "            word = np.array(model.get_vector(word))\n",
    "            word = np.append(word, 1)\n",
    "            v =  np.dot(t, word)[0:-1]\n",
    "            return v / np.linalg.norm(v)\n",
    "    return model.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WEATtest(embedding_meth, T, classes, biasToBeTested, threshold = thresh):\n",
    "    d_values = []\n",
    "    for category_target_pair in biasToBeTested:\n",
    "        cat_0 = [embedding_meth(c,classes,T) for c in sets[category_target_pair[0]]]\n",
    "        cat_1 = [embedding_meth(c,classes,T) for c in sets[category_target_pair[1]]]\n",
    "        for attribute_pair in biasToBeTested[category_target_pair]:\n",
    "            att_0 = [embedding_meth(a,classes,T) for a in sets[attribute_pair[0]]]\n",
    "            att_1 = [embedding_meth(a,classes,T) for a in sets[attribute_pair[1]]]\n",
    "            d = WEAT_Effect_size(cat_0, cat_1, att_0, att_1)\n",
    "            d_values.append(abs(d))\n",
    " \n",
    "    return np.mean(d_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getT(debias_dict, all_attribute_words, classes, l):\n",
    "    t = {'islam_words': [], 'christianity_words':[], 'judaism_words':[]}\n",
    "    final_t = {}\n",
    "    #for each subclass\n",
    "    for s in debias_dict.keys():\n",
    "        biasedAttr = debias_dict[s]\n",
    "        #for each attribute set its biased against\n",
    "        print(biasedAttr, type(biasedAttr))\n",
    "        targetWords = classes[s]  # target words are all terms referring to the class and their neighbours determined earlier\n",
    "        #if a word is both in targetwords and attributes, then delete from targetwords\n",
    "        for w in all_attribute_words:\n",
    "            if w in targetWords:\n",
    "                targetWords.remove(w)\n",
    "        #embed targetwords\n",
    "        targetWords = [model.get_vector(w) for w in targetWords if w in model.vocab]\n",
    "        #mean of target words\n",
    "        meanTargetWord = np.mean(targetWords, axis = 0)\n",
    "        #for each biased attribute set, calculate mean of attribute set\n",
    "        meanAttSets = []\n",
    "        for attset in biasedAttr:\n",
    "            tempSet = [model.get_vector(w) for w in sets[attset]]\n",
    "            meanAttSets.append(np.mean(tempSet, axis = 0))\n",
    "        meanMatrix = np.array(meanAttSets)\n",
    "        #null space vectors for mean of attribute sets:\n",
    "        #The null space of any matrix A consists of all the vectors B such that AB = 0 and B is not zero\n",
    "        null_space_vectors = ns(meanMatrix)\n",
    "        print('len', np.size(null_space_vectors, 1))\n",
    "        temp = 100#size(null_space_vectors, axis = 0)\n",
    "        print(type(null_space_vectors))\n",
    "        d_vals = []\n",
    "        best = 2\n",
    "        bestMat = None\n",
    "        for n in range(temp):\n",
    "            #n is the number null space vector \n",
    "            T = null_space_vectors[:,n] - meanTargetWord\n",
    "            TI = np.identity(len(T)+1)\n",
    "            for t_i, value in enumerate(T):\n",
    "                TI[t_i, -1] = value * l \n",
    "            t[s] = TI\n",
    "            d = WEATtest(embeddingsoftWEAT,t,classes, religion)\n",
    "            #print('effect size d = ',d) \n",
    "            if d < best: \n",
    "                print('changed d is now ',d)\n",
    "                best = d\n",
    "                bestMat = TI\n",
    "            d_vals.append(d)\n",
    "        final_t[s] = TI\n",
    "    return final_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softWEAT(lamda):\n",
    "    #get attribute sets to debias to\n",
    "    debias_dict,_ = WEAT(encode_nondebias,religion)\n",
    "    print(debias_dict)\n",
    "    \n",
    "    #establish target words and neighbours to debias\n",
    "    classes = {}\n",
    "    classes['islam_words'] = sets['islam_words'].copy()\n",
    "    classes['christianity_words'] = sets['christianity_words'].copy()\n",
    "    classes['judaism_words'] = sets['judaism_words'].copy()\n",
    "    classes = addNeighbourWords(classes, 10)\n",
    "    for s in classes:\n",
    "        print(s, len(classes[s]))\n",
    "        \n",
    "    #list all words in each attribute set\n",
    "    all_attribute_words = []\n",
    "    for s in sets.keys():\n",
    "        if s != 'christianity_words' and s!= 'islam_words' and s!= 'judaism_words':\n",
    "            all_attribute_words.extend(sets[s])\n",
    "    print('All Attribute Words', all_attribute_words)\n",
    "    \n",
    "    final_t = {}\n",
    "    final_t = getT(debias_dict, all_attribute_words, classes,lamda)\n",
    "    return final_t, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likeable , unlikeable, islam_words, judaism_words = -0.26066891607626996\n",
      "competent , incompetent, islam_words, judaism_words = 0.40838164801997817\n",
      "shy , aggressive, islam_words, judaism_words = -0.005682144845930048\n",
      "intellectual_words , appearance_words, islam_words, judaism_words = -0.4421097319788695\n",
      "family , career, islam_words, judaism_words = -0.4786866281807168\n",
      "instruments , weapons, islam_words, judaism_words = 1.2101514554789525\n",
      "pleasant , unpleasant, islam_words, judaism_words = 0.9100249979993442\n",
      "science , art, islam_words, judaism_words = -0.41257207784203753\n",
      "likeable , unlikeable, islam_words, christianity_words = -0.28454531118961734\n",
      "competent , incompetent, islam_words, christianity_words = 0.7620611940494734\n",
      "shy , aggressive, islam_words, christianity_words = -0.030062573246757235\n",
      "intellectual_words , appearance_words, islam_words, christianity_words = -0.2772419557116415\n",
      "family , career, islam_words, christianity_words = -0.30306180801277566\n",
      "instruments , weapons, islam_words, christianity_words = 0.7599421809029535\n",
      "pleasant , unpleasant, islam_words, christianity_words = 1.0536442159511894\n",
      "science , art, islam_words, christianity_words = -0.31651217215973515\n",
      "likeable , unlikeable, judaism_words, christianity_words = 0.018352683771000064\n",
      "competent , incompetent, judaism_words, christianity_words = 0.2567078594016816\n",
      "shy , aggressive, judaism_words, christianity_words = -0.023672850382630804\n",
      "intellectual_words , appearance_words, judaism_words, christianity_words = 0.17404635457394793\n",
      "family , career, judaism_words, christianity_words = 0.15471786708628507\n",
      "instruments , weapons, judaism_words, christianity_words = -0.8087072262644193\n",
      "pleasant , unpleasant, judaism_words, christianity_words = 0.041342490092856066\n",
      "science , art, judaism_words, christianity_words = 0.07971854132974603\n",
      "{'islam_words': ['instruments', 'pleasant', 'competent'], 'christianity_words': ['incompetent', 'weapons', 'unpleasant', 'instruments'], 'judaism_words': ['weapons', 'unpleasant']}\n",
      "islam_words 91\n",
      "['islam', 'muslim', 'muslims', 'mosque', 'mosques', 'quran', 'imam', 'imams', 'muhammad', 'hilal', 'ramadan', 'islamic', 'Islam', 'radical_islam', 'religon', 'hinduism', 'hindu', 'kafir', 'Muslim', 'Moslem', 'moslem', 'Muslims', 'Moslems', 'moslems', 'pakistanis', 'tamils', 'Mosque', 'masjid', 'Masjid', 'mosque_imam', 'Mosques', 'Haqq_mosque', 'mosque_Masjid', 'Noor_Mosque', 'Churches_synagogues', 'churches_synagogues_temples', 'masjids', 'Masjids', 'gurdwaras', 'Eidgahs', 'koran', \"Qur'aan\", \"Qur'an\", \"Qu'ran\", \"holy_Qur'an\", 'Quoran', 'surahs', \"Q'uran\", \"Noble_Qur'an\", 'holy_Koran', 'Imam', 'Muslim_cleric', 'cleric', 'Masjid_al', 'Imam_Siraj_Wahhaj', 'Imam_Sayed', 'Imams', 'Muslim_clerics', 'radical_imams', 'clerics', 'Islamic_clerics', 'preachers', 'mohammed', 'imran', 'mohammad', 'ahmed', 'irfan', 'pir', 'syed', 'Mohammed_PBUH', '@_courier.co.uk', 'Shawwal', 'Hijri_calendar', 'Shawwal_moon', 'Syawal', 'astronomical_calculations', 'Shawal_moon', 'Dhul_Hijjah', 'Hijri', 'Dhul_Hijja', 'crescent_moon', 'fitr', 'puasa', 'Eid_Ul', 'kenduri', 'Ramzaan', 'eid', 'Ramadaan', 'Maghrib_prayer', 'Isyak_prayers', 'adha']\n",
      "christianity_words 86\n",
      "['christianity', 'christian', 'christians', 'church', 'churches', 'bible', 'priest', 'priests', 'jesus', 'cross', 'easter', 'Christianity', 'athiests', 'mormons', 'athiest', 'catholicism', 'relgion', 'christain', 'baptist', 'mormon', 'catholics', 'Christians', 'Christains', 'bible_thumpers', 'africans', 'congregation', 'Church', 'parishioners', 'Catholic_Church', 'congregations', 'parish', 'chuch', 'parishoners', 'Roman_Catholic_Church', 'Churches', 'pastors', 'Protestant_churches', 'Baptist_churches', 'Evangelical_Covenant', 'Presbyterian_congregations', 'parachurch_ministries', 'parishes', 'Bible', 'bibles', 'Holy_Bible', 'scriptures', 'scripture', 'New_Testament', 'Scripture', 'Didache', 'Scriptures', 'Oxford_Annotated', 'Catholic_priest', 'parish_priest', 'bishop', 'Roman_Catholic_priest', 'archbishop', 'seminarian', 'clergyman', 'diocesan_priest', 'catholic_priest', 'clergy', 'bishops', 'parish_priests', 'seminarians', 'diocesan_priests', 'Roman_Catholic_priests', 'diocesan_clergy', 'diocese', 'jesus_christ', 'christ', 'cuz_u', 'jesse', 'lmfao', 'hahah', 'angelina', 'hitler', 'michele', 'crosses', 'crossed', 'crossing', 'Selembu', 'corss', 'christmas', 'Easter', 'xmas']\n",
      "judaism_words 101\n",
      "['judaism', 'jew', 'jews', 'synagogue', 'synagogues', 'torah', 'rabbi', 'rabbis', 'abraham', 'star', 'shabbat', 'jewish', 'jessie', 'licencia_photoshop_cs4', 'ayin', 'licencia_microsoft_office', 'acheter_nero', 'zionism', 'fsf', 'licencia_adobe_photoshop', 'shari', 'lingua', 'prix_acrobat', 'rahm', 'mhux', 'yid', 'yur', 'israel', 'gilbert', 'kol', 'arabs', 'Jews', 'palestinians', 'zionists', 'palestine', 'kikes', 'shul', 'Synagogue', 'Orthodox_synagogue', 'shuls', 'Orthodox_shul', 'Chabad_rabbi', 'Kehilat', 'yeshiva', 'Synagogues', 'Jewish', 'Bet_Haverim', 'JCCs', 'mikvahs', 'Torah', 'siddur', 'Torah_scroll', 'sefer_Torah', 'Mishkan', 'Sefer_Torah', 'haftarah', 'sefer', 'Rabbi_Youlus', 'mitzvah', 'Rabbi', 'rabbinical', 'Orthodox_rabbi', 'cantor', 'Rabbi_Yosef', 'rabbinic', 'Rabbis', 'Orthodox_rabbis', 'rabbonim', 'rabbinate', 'halachic', 'Zionist_rabbis', 'armstrong', 'simon', 'derek', 'caroline', 'nigel', 'adrian', 'maxwell', 'cherie', 'nuffin', 'vivian', 'stars', 'superstar', 'starlet', 'megastar', 'heart_throb', 'teen_heart_throb', 'heart_throb_Zac_Efron', 'heartthrob', 'superstars', 'standout', 'oneg', 'havdalah', 'kiddush', 'Shavuos', 'Shabbat', 'mincha', 'Erev_Shabbat', 'Shabbat_Sabbath', 'Shemini_Atzeret', 'seudah']\n",
      "islam_words 91\n",
      "christianity_words 86\n",
      "judaism_words 101\n",
      "All Attribute Words ['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise', 'family', 'happy', 'laughter', 'paradise', 'vacation', 'joy', 'wonderful', 'abuse', 'crash', 'filth', 'murder', 'sickness', 'accident', 'death', 'grief', 'poison', 'stink', 'assault', 'disaster', 'hatred', 'pollute', 'tragedy', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'kill', 'rotten', 'vomit', 'agony', 'prison', 'terrible', 'horrible', 'bagpipe', 'cello', 'guitar', 'lute', 'trombone', 'banjo', 'clarinet', 'harmonica', 'mandolin', 'trumpet', 'bassoon', 'drum', 'harp', 'oboe', 'tuba', 'bell', 'fiddle', 'harpsichord', 'piano', 'viola', 'bongo', 'flute', 'horn', 'saxophone', 'arrow', 'club', 'gun', 'missile', 'spear', 'dagger', 'harpoon', 'pistol', 'sword', 'blade', 'dynamite', 'hatchet', 'rifle', 'tank', 'bomb', 'firearm', 'knife', 'shotgun', 'teargas', 'cannon', 'grenade', 'mace', 'slingshot', 'whip', 'executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career', 'home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives', 'science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment', 'astronomy', 'poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture', 'resourceful', 'inquisitive', 'sagacious', 'inventive', 'adaptable', 'intuitive', 'analytical', 'imaginative', 'shrewd', 'thoughtful', 'smart', 'clever', 'brilliant', 'logical', 'intelligent', 'apt', 'genius', 'wise', 'stupid', 'dumb', 'dull', 'clumsy', 'foolish', 'naive', 'unintelligent', 'alluring', 'voluptuous', 'blushing', 'homely', 'plump', 'sensual', 'gorgeous', 'slim', 'bald', 'athletic', 'fashionable', 'stout', 'ugly', 'muscular', 'slender', 'feeble', 'handsome', 'healthy', 'attractive', 'fat', 'weak', 'thin', 'pretty', 'beautiful', 'strong', 'soft', 'quiet', 'compromising', 'rational', 'calm', 'kind', 'agreeable', 'servile', 'pleasant', 'cautious', 'friendly', 'supportive', 'nice', 'mild', 'demure', 'passive', 'indifferent', 'shrill', 'loud', 'argumentative', 'irrational', 'angry', 'abusive', 'obnoxious', 'controlling', 'nagging', 'brash', 'hostile', 'mean', 'harsh', 'sassy', 'aggressive', 'opinionated', 'domineering', 'competent', 'productive', 'effective', 'ambitious', 'active', 'decisive', 'strong', 'tough', 'incompetent', 'unproductive', 'ineffective', 'passive', 'indecisive', 'weak', 'gentle', 'timid', 'agreeable', 'fair', 'honest', 'trustworthy', 'selfless', 'accommodating', 'abrasive', 'conniving', 'manipulative', 'dishonest', 'selfish', 'pushy']\n",
      "['instruments', 'pleasant', 'competent'] <class 'list'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 297\n",
      "<class 'numpy.ndarray'>\n",
      "changed d is now  0.38822100164802914\n",
      "changed d is now  0.33643090579389545\n",
      "changed d is now  0.3353470144982699\n",
      "changed d is now  0.3105159380964588\n",
      "changed d is now  0.27597121635468186\n",
      "['incompetent', 'weapons', 'unpleasant', 'instruments'] <class 'list'>\n",
      "len 296\n",
      "<class 'numpy.ndarray'>\n",
      "changed d is now  0.3814595917335346\n",
      "changed d is now  0.3571336603699085\n",
      "changed d is now  0.32677588210800407\n",
      "['weapons', 'unpleasant'] <class 'list'>\n",
      "len 298\n",
      "<class 'numpy.ndarray'>\n",
      "changed d is now  0.27254919217664975\n",
      "changed d is now  0.2677468156619666\n",
      "changed d is now  0.2630272128622321\n",
      "changed d is now  0.259195572015313\n",
      "changed d is now  0.2524655944101561\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.5\n",
    "final_t, classes = softWEAT(lamda= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddingsoftWEATEval(word):\n",
    "    T = final_t\n",
    "    if word not in model.vocab: \n",
    "        return 'None'\n",
    "    for sc in classes.keys():\n",
    "        if word in classes[sc]:\n",
    "            if T[sc] != []:\n",
    "                t = T[sc]\n",
    "                word = np.array(model.get_vector(word))\n",
    "                word = np.append(word, 1)\n",
    "                v =  np.dot(t, word)[0:-1]\n",
    "                return v / np.linalg.norm(v)\n",
    "    v = model.get_vector(word)\n",
    "    return v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judaism  0.6162357087370495 christ 0.6474713033764599  islam 0.7634398968088492\n",
      "judaism  0.5765224928399951 christ 0.6707283255974406  islam 0.8555225942861951\n",
      "average over 20  0.11610576432784316 [0.00273448 0.04396369 0.0030257  0.0065152  0.01959617 0.51477097\n",
      " 0.17519754 0.05360199 0.03017941 0.0951017  0.33247655]\n",
      "test stat -1.9901727314249595\n",
      "p  0.037299573697205775\n",
      "average over 20  0.07296111003833436 [6.97237413e-05 2.29237121e-02 9.71506273e-03 1.01376681e-02\n",
      " 1.78188363e-02 2.80638855e-01 3.52233891e-02 1.18326112e-02\n",
      " 1.25789426e-02 7.82369741e-02 3.23396435e-01]\n",
      "['judaism', 'christianity', 'islam'] : 0.8088369943270665\n",
      "['jew', 'christian', 'muslim'] : 0.7958037788388955\n",
      "['jews', 'christians', 'muslims'] : 0.8308680761094593\n",
      "['synagogue', 'church', 'mosque'] : 1.0385256984294269\n",
      "['synagogues', 'churches', 'mosques'] : 1.0233995099758033\n",
      "['torah', 'bible', 'quran'] : 0.888129615504949\n",
      "['rabbi', 'priest', 'imam'] : 1.0444749224309107\n",
      "['rabbis', 'priests', 'imams'] : 1.0320160697984597\n",
      "['abraham', 'jesus', 'muhammad'] : 0.799691615823864\n",
      "['star', 'cross', 'hilal'] : 1.0100354915675094\n",
      "['shabbat', 'easter', 'ramadan'] : 0.8740125060491254\n",
      "['judaism', 'christianity', 'islam'] : 0.7798762041004368\n",
      "['jew', 'christian', 'muslim'] : 0.7779697438448442\n",
      "['jews', 'christians', 'muslims'] : 0.8117313206891456\n",
      "['synagogue', 'church', 'mosque'] : 0.9913475976506388\n",
      "['synagogues', 'churches', 'mosques'] : 0.9812809082575095\n",
      "['torah', 'bible', 'quran'] : 0.8595037080193201\n",
      "['rabbi', 'priest', 'imam'] : 0.9981623588407634\n",
      "['rabbis', 'priests', 'imams'] : 0.9899504226712583\n",
      "['abraham', 'jesus', 'muhammad'] : 0.7449998714863378\n",
      "['star', 'cross', 'hilal'] : 0.9370946496251209\n",
      "['shabbat', 'easter', 'ramadan'] : 0.8314679134777648\n",
      "score: 0.8821258816966492\n",
      "test stat 39.749015543495865\n",
      "p  5.1805721086193794e-161\n",
      "score: 0.9223449344414063\n",
      "[0.0431177358337142, 0.41754938481546583, 0.08319347128664856, 0.12704618135329238, 0.5762235694759599, 0.8340100581605826, 0.7593488860338011, 0.42506179652982906, 0.01385025216652964, 0.5119903132165662, 0.2722481453313275, 0.008175385359469226, 0.34016222970613574, 0.4401232774530972, 0.9655917053958134, 0.07427945559921056, 0.03062756949337663, 0.07174282676549698, 0.18891638684824041, 0.11072399774881578, 0.2728012388346777, 0.5443654992135015, 0.13497837233940177, 0.34723768704066776]\n"
     ]
    }
   ],
   "source": [
    "relBias,_,p  = RNSB_Evaluation(embeddingsoftWEATEval,equality_sets)\n",
    "score,p2,t = multiclass_evaluation(embeddingsoftWEATEval)\n",
    "Results['softWEAT_Debias*'] = [relBias,p, score, p2,WEAT_Average_Effect_size(embeddingsoftWEATEval, religion)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nondebias* [0.12823775556463923, 0, 0.8821258816966492, nan, 0.39469228685620034]\n",
      "Conceptor_Debias* [0.0067377115121234, 0.018414037101185162, 0.9956381136554976, 1.4359195545117307e-103, 0.17112280499191349]\n",
      "Hard_Debias* [6.99909438110394e-07, 0.01857421168741203, 0.8896109938367172, 4.410203498887944e-08, 0.0008266102336912121]\n",
      "softWEAT_Debias* [0.07296111003833436, 0.037299573697205775, 0.9223449344414063, 5.1805721086193794e-161, 0.3163902260834009]\n"
     ]
    }
   ],
   "source": [
    "for i in Results.keys():\n",
    "    print(i, Results[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1TfjLD5e4MK"
   },
   "source": [
    "# Visualization Of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nondebias*\n",
      "RNSB Score: 0.12824 with a p value of  0;\n",
      "MAC Score: 0.88213; with a p value of nan\n",
      "WEAT Score: 0.39469 \n",
      "Conceptor_Debias*\n",
      "RNSB Score: 0.00674 with a p value of  0.01841;\n",
      "MAC Score: 0.99564; with a p value of 0.0\n",
      "WEAT Score: 0.17112 \n",
      "Hard_Debias*\n",
      "RNSB Score: 0.0 with a p value of  0.01857;\n",
      "MAC Score: 0.88961; with a p value of 0.0\n",
      "WEAT Score: 0.00083 \n",
      "softWEAT_Debias*\n",
      "RNSB Score: 0.07296 with a p value of  0.0373;\n",
      "MAC Score: 0.92234; with a p value of 0.0\n",
      "WEAT Score: 0.31639 \n"
     ]
    }
   ],
   "source": [
    "for key, value in Results.items():\n",
    "    print(key)\n",
    "    print(f'RNSB Score: {round(value[0],5)} with a p value of  {round(value[1],5)};')\n",
    "    print(f'MAC Score: {round(value[2],5)}; with a p value of {round(value[3],5)}')      \n",
    "    print(f'WEAT Score: {round(value[4],5)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TvnjfiLCyfIX",
    "outputId": "a53216fa-e1cc-471d-abb4-1424f7fbf90c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nondebias* : 0.12823775556463923\n",
      " Conceptor_Debias* : 0.0067377115121234\n",
      " Hard_Debias* : 6.99909438110394e-07\n",
      " softWEAT_Debias* : 0.07296111003833436\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RNSB = [d[0] for d in Results.values()]\n",
    "for key, value in Results.items():\n",
    "    print(f' {key} : {value[0]}')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.title('RNSB on Judaism, Christianity, Islam')\n",
    "plt.bar(Results.keys(), RNSB)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZ6UF9O2yfIg",
    "outputId": "cfd46c61-469a-4951-f2a3-3bdf66bc00ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nondebias* : 0\n",
      " Conceptor_Debias* : 0.018414037101185162\n",
      " Hard_Debias* : 0.01857421168741203\n",
      " softWEAT_Debias* : 0.037299573697205775\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAC = [d[2] for d in Results.values()]\n",
    "for key, value in Results.items():\n",
    "    print(f' {key} : {value[1]}')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.title('Mean Average Cosine Similarity')\n",
    "plt.bar(Results.keys(), MAC)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFZgVmeMyfIi",
    "outputId": "c11e291e-505f-4c49-e649-d46ee7fbd4a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nondebias* : 0.8821258816966492\n",
      " Conceptor_Debias* : 0.9956381136554976\n",
      " Hard_Debias* : 0.8896109938367172\n",
      " softWEAT_Debias* : 0.9223449344414063\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WEATD = [d[4] for d in Results.values()]\n",
    "for key, value in Results.items():\n",
    "    print(f' {key} : {value[2]}')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.title('WEAT Effect size')\n",
    "plt.bar(Results.keys(), WEATD)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CBslGgawe4KP",
    "GniwlqaTe4Ki",
    "zTXrSxGpe4Lq"
   ],
   "name": "Thesis_Planning(ii).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (Thesis Code)",
   "language": "python",
   "name": "pycharm-8c054aba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
